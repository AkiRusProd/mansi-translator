{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:43:04.195895Z",
     "start_time": "2024-09-10T18:42:56.132566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vladimir\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import gc\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    losses\n",
    ")\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TranslationEvaluator\n",
    "from transformers.integrations import TensorBoardCallback  \n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "385393c1a4a1d494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:43:14.066931Z",
     "start_time": "2024-09-10T18:43:10.195729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(501710, 768, padding_idx=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "tokenizer = AutoTokenizer.from_pretrained('C:/Users/Vladimir/PycharmProjects/perevod/mansi-translator/data/tokenizers/tokenizer_v1')\n",
    "model.tokenizer = tokenizer\n",
    "model._first_module().auto_model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d0175aab1b6cb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:43:18.845099Z",
     "start_time": "2024-09-10T18:43:18.826863Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    return None\n",
    "\n",
    "\n",
    "setup_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18635a9d3ad25f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:43:24.435784Z",
     "start_time": "2024-09-10T18:43:21.360582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "      <th>similarities</th>\n",
       "      <th>negative_examples_mans</th>\n",
       "      <th>negative_examples_rus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Та пыгрисит маим вармаль э̄рнэ поратэт ат верм...</td>\n",
       "      <td>Те мальчики не выполнят задание в назначенный ...</td>\n",
       "      <td>{0: 1.0, 52122: 0.9042518734931946, 18930: 0.8...</td>\n",
       "      <td>Пыгрищит ка̄таныл иӈ ва̄гталыт, ты та̄рвитыӈ р...</td>\n",
       "      <td>У мальчиков еще нет сил выполнять тяжелую рабо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ха̄йтыматэ тӯр ва̄тан ёхтыс, вит ва̄тан ха̄йтыс.</td>\n",
       "      <td>Бегая к берегу озера пришла, к воде подбежала.</td>\n",
       "      <td>{1: 0.9999999403953552, 4463: 0.97095388174057...</td>\n",
       "      <td>Ха̄йтыме̄т ос тӯр ва̄та ка̄салас, тув ха̄йтыс.</td>\n",
       "      <td>Бежала, увидела берег озера, туда направилась.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Вит са̄мыл сунсым о̄нтыс</td>\n",
       "      <td>Вода прибывала на глазах</td>\n",
       "      <td>{2: 0.9999999403953552, 33249: 0.9213534593582...</td>\n",
       "      <td>Вӣт ветраныл ма̄н сюргыс.</td>\n",
       "      <td>Вода текла на землю.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Атаявев, акваг лылынг тагл ворн та тотавев.</td>\n",
       "      <td>Обнюхивает нас, живыми на кладбище уносит.</td>\n",
       "      <td>{3: 0.9999997019767761, 12914: 0.9601884484291...</td>\n",
       "      <td>Атаяве̄в, акваг лылыӈ та̄гыл во̄рн та тотаве̄в</td>\n",
       "      <td>Она обнюхивает нас и на кладбище уносит</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ман ты пӣлтал, веськат хумиюв нэтхуньт ат ёру...</td>\n",
       "      <td>Мы никогда не забудем этого честного человека.</td>\n",
       "      <td>{4: 1.0, 55368: 0.9368814826011658, 61451: 0.9...</td>\n",
       "      <td>Ёмас хо̄тпа хотта мус номуӈкве патылӯв.</td>\n",
       "      <td>Хорошего человека будем помнить долго.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              target  \\\n",
       "0  Та пыгрисит маим вармаль э̄рнэ поратэт ат верм...   \n",
       "1  Ха̄йтыматэ тӯр ва̄тан ёхтыс, вит ва̄тан ха̄йтыс.   \n",
       "2                           Вит са̄мыл сунсым о̄нтыс   \n",
       "3        Атаявев, акваг лылынг тагл ворн та тотавев.   \n",
       "4  Ман ты пӣлтал, веськат хумиюв нэтхуньт ат ёру...   \n",
       "\n",
       "                                              source  \\\n",
       "0  Те мальчики не выполнят задание в назначенный ...   \n",
       "1     Бегая к берегу озера пришла, к воде подбежала.   \n",
       "2                           Вода прибывала на глазах   \n",
       "3         Обнюхивает нас, живыми на кладбище уносит.   \n",
       "4     Мы никогда не забудем этого честного человека.   \n",
       "\n",
       "                                        similarities  \\\n",
       "0  {0: 1.0, 52122: 0.9042518734931946, 18930: 0.8...   \n",
       "1  {1: 0.9999999403953552, 4463: 0.97095388174057...   \n",
       "2  {2: 0.9999999403953552, 33249: 0.9213534593582...   \n",
       "3  {3: 0.9999997019767761, 12914: 0.9601884484291...   \n",
       "4  {4: 1.0, 55368: 0.9368814826011658, 61451: 0.9...   \n",
       "\n",
       "                              negative_examples_mans  \\\n",
       "0  Пыгрищит ка̄таныл иӈ ва̄гталыт, ты та̄рвитыӈ р...   \n",
       "1    Ха̄йтыме̄т ос тӯр ва̄та ка̄салас, тув ха̄йтыс.   \n",
       "2                         Вӣт ветраныл ма̄н сюргыс.   \n",
       "3     Атаяве̄в, акваг лылыӈ та̄гыл во̄рн та тотаве̄в   \n",
       "4           Ёмас хо̄тпа хотта мус номуӈкве патылӯв.   \n",
       "\n",
       "                               negative_examples_rus  \n",
       "0  У мальчиков еще нет сил выполнять тяжелую рабо...  \n",
       "1     Бежала, увидела берег озера, туда направилась.  \n",
       "2                               Вода текла на землю.  \n",
       "3            Она обнюхивает нас и на кладбище уносит  \n",
       "4             Хорошего человека будем помнить долго.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data_train.csv')\n",
    "dataset['similarities'] = dataset['similarities'].apply(literal_eval)\n",
    "dataset = dataset.loc[dataset['target'].drop_duplicates(keep='last').index]\n",
    "dataset = dataset.loc[dataset['source'].drop_duplicates(keep='last').index]\n",
    "dataset.dropna(inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd092467cbbeba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(dataset, test_size=7_500, random_state=777, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e325ded0-3d31-48df-95e7-6dadbbd0fdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62772, 5), (7500, 5))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8697b71e-036d-441f-872d-c1143a2e988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"anchor\": train_set['source'].to_list(),\n",
    "        \"positive\": train_set['target'].to_list(),\n",
    "        \"negative\": train_set['negative_examples_mans'].to_list()\n",
    "    }\n",
    ")\n",
    "\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"anchor\": test_set['source'].to_list(),\n",
    "        \"positive\": test_set['target'].to_list(),\n",
    "        \"negative\": test_set['negative_examples_mans'].to_list()\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17dbecbcdc23f75f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T12:56:52.692903Z",
     "start_time": "2024-09-10T12:56:49.707839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfroze embeddings.word_embeddings.weight\n",
      "Unfroze embeddings.position_embeddings.weight\n",
      "Unfroze embeddings.token_type_embeddings.weight\n",
      "Unfroze embeddings.LayerNorm.weight\n",
      "Unfroze embeddings.LayerNorm.bias\n",
      "Unfroze encoder.layer.11.attention.self.query.weight\n",
      "Unfroze encoder.layer.11.attention.self.query.bias\n",
      "Unfroze encoder.layer.11.attention.self.key.weight\n",
      "Unfroze encoder.layer.11.attention.self.key.bias\n",
      "Unfroze encoder.layer.11.attention.self.value.weight\n",
      "Unfroze encoder.layer.11.attention.self.value.bias\n",
      "Unfroze encoder.layer.11.attention.output.dense.weight\n",
      "Unfroze encoder.layer.11.attention.output.dense.bias\n",
      "Unfroze encoder.layer.11.attention.output.LayerNorm.weight\n",
      "Unfroze encoder.layer.11.attention.output.LayerNorm.bias\n",
      "Unfroze encoder.layer.11.output.dense.weight\n",
      "Unfroze encoder.layer.11.output.dense.bias\n",
      "Unfroze encoder.layer.11.output.LayerNorm.weight\n",
      "Unfroze encoder.layer.11.output.LayerNorm.bias\n",
      "Unfroze pooler.dense.weight\n",
      "Unfroze pooler.dense.bias\n",
      "Unfroze Dense layer: linear.weight\n",
      "Unfroze Dense layer: linear.bias\n"
     ]
    }
   ],
   "source": [
    "class LaBSEFineTuner:\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.bert_model = model._first_module().auto_model\n",
    "        \n",
    "        self.pooling = self.model[1]\n",
    "        self.dense = self.model[2]\n",
    "        self.normalize = self.model[3]\n",
    "        \n",
    "        self.unfreeze_before_attention()\n",
    "        self.unfreeze_last_head()\n",
    "        self.unfreeze_pooling_dense_normalize()\n",
    "\n",
    "    def unfreeze_before_attention(self):\n",
    "        for name, param in self.bert_model.named_parameters():\n",
    "            if 'encoder.layer.0.attention' in name:\n",
    "                break\n",
    "            param.requires_grad = True\n",
    "            print(f\"Unfroze {name}\")\n",
    "\n",
    "    def unfreeze_last_head(self):\n",
    "        for name, param in self.bert_model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        last_layer_idx = len(self.bert_model.encoder.layer) - 1\n",
    "        for name, param in self.bert_model.named_parameters():\n",
    "            if f'encoder.layer.{last_layer_idx}.attention' in name or f'encoder.layer.{last_layer_idx}.output' in name:\n",
    "                param.requires_grad = True\n",
    "                print(f\"Unfroze {name}\")\n",
    "\n",
    "        for name, param in self.bert_model.named_parameters():\n",
    "            if 'pooler' in name or 'cls' in name:\n",
    "                param.requires_grad = True\n",
    "                print(f\"Unfroze {name}\")\n",
    "                \n",
    "    def unfreeze_pooling_dense_normalize(self):\n",
    "        for name, param in self.pooling.named_parameters():\n",
    "            param.requires_grad = True\n",
    "            print(f\"Unfroze Pooling layer: {name}\")\n",
    "        \n",
    "        for name, param in self.dense.named_parameters():\n",
    "            param.requires_grad = True\n",
    "            print(f\"Unfroze Dense layer: {name}\")\n",
    "        \n",
    "        for name, param in self.normalize.named_parameters():\n",
    "            param.requires_grad = True\n",
    "            print(f\"Unfroze Normalize layer: {name}\")\n",
    "        \n",
    "    def count_trainable_params(self):\n",
    "        trainable_params = sum(p.numel() for p in self.bert_model.parameters() if p.requires_grad)\n",
    "        trainable_params += sum(p.numel() for p in self.pooling.parameters() if p.requires_grad)\n",
    "        trainable_params += sum(p.numel() for p in self.dense.parameters() if p.requires_grad)\n",
    "        trainable_params += sum(p.numel() for p in self.normalize.parameters() if p.requires_grad)\n",
    "        return print(f\"Trainable parameters: {trainable_params}\")\n",
    "    \n",
    "    def to(self, device):\n",
    "        return self.bert_model.to(device=device)\n",
    "\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "LaBSE = LaBSEFineTuner(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dfd169a28f9d10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T13:17:55.797361Z",
     "start_time": "2024-09-10T13:17:55.793856Z"
    }
   },
   "outputs": [],
   "source": [
    "assert sum([p.numel() for _, p in LaBSE.model[2].named_parameters() if p.requires_grad == True]) == 590592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10a7e07210c0cdf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T13:21:13.694957Z",
     "start_time": "2024-09-10T13:21:13.688837Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [\n",
    "            *LaBSE.model[0].auto_model.embeddings.word_embeddings.parameters(),\n",
    "            *LaBSE.model[0].auto_model.embeddings.position_embeddings.parameters(),\n",
    "            *LaBSE.model[0].auto_model.embeddings.token_type_embeddings.parameters(),\n",
    "            *LaBSE.model[0].auto_model.embeddings.LayerNorm.parameters()\n",
    "        ],\n",
    "        'lr': 5e-3,\n",
    "        'weight_decay': 0.05\n",
    "    },\n",
    "    {\n",
    "        'params': LaBSE.model[0].auto_model.encoder.layer[11].parameters(),\n",
    "        'lr': 3e-5,\n",
    "        'weight_decay': 0.01\n",
    "    },\n",
    "    {\n",
    "        'params': LaBSE.model[0].auto_model.pooler.parameters(),\n",
    "        'lr': 3e-5,\n",
    "        'weight_decay': 0.01\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, eps=1e-8)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.08,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87981884-e2e8-41bb-b4ab-6a7cbf4f6104",
   "metadata": {},
   "source": [
    "# Конфигурация обучения \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae2a3879-ab4b-486f-9066-4e86f79a734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = losses.CachedMultipleNegativesRankingLoss(model=LaBSE.model, mini_batch_size=128).to(device='cuda:0')\n",
    "\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"models\",  \n",
    "    num_train_epochs=30,  \n",
    "    per_device_train_batch_size=64,  \n",
    "    per_device_eval_batch_size=64, \n",
    "    max_grad_norm=1.1,\n",
    "    # lr_scheduler_type=transformers.get_cosine_schedule_with_warmup(\n",
    "    #     optimizer, \n",
    "    #     num_warmup_steps=len(train_dataset) // 64, \n",
    "    #     num_training_steps=len(train_dataset) * 30\n",
    "    # ),\n",
    "    restore_callback_states_from_checkpoint=True,\n",
    "    load_best_model_at_end=True,\n",
    "    use_cpu=False,\n",
    "    seed=777,\n",
    "    fp16=False,  \n",
    "    bf16=False,\n",
    "    disable_tqdm=False,\n",
    "    remove_unused_columns=True,\n",
    "    greater_is_better=True,\n",
    "    metric_for_best_model='eval_Perevod_mean_accuracy',\n",
    "    ignore_data_skip=False,\n",
    "    # dataloader_persistent_workers=True,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  \n",
    "    eval_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\",  \n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "evaluator = TranslationEvaluator(\n",
    "    source_sentences=test_dataset[\"anchor\"],\n",
    "    show_progress_bar=True,\n",
    "    target_sentences=test_dataset[\"positive\"],\n",
    "    name=\"Perevod\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0546008-1ac2-42d5-b500-116f88fc7a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1114' max='29430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1114/29430 06:41 < 2:50:27, 2.77 it/s, Epoch 1.13/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perevod Src2trg Accuracy</th>\n",
       "      <th>Perevod Trg2src Accuracy</th>\n",
       "      <th>Perevod Mean Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.468800</td>\n",
       "      <td>1.250345</td>\n",
       "      <td>0.363733</td>\n",
       "      <td>0.310533</td>\n",
       "      <td>0.337133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d443310683ac4588a4b5f6a63e325caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bc9f4bed3746e68f6ec321b6d7e9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e856280dd4eb436695a75553b9522117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    loss=loss_function,\n",
    "    evaluator=evaluator,\n",
    "    # callbacks=[TensorBoardCallback(SummaryWriter(log_dir=\"logs/tensorboard\"))]  \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22893f30-aee0-4b23-a6b1-1165af02e0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa9131c89984951b6198bd02cdce7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8804337fd7da4cbebf7186843998f43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available metrics:\n",
      "eval_loss\n",
      "eval_Perevod_src2trg_accuracy\n",
      "eval_Perevod_trg2src_accuracy\n",
      "eval_Perevod_mean_accuracy\n"
     ]
    }
   ],
   "source": [
    "def evaluate_and_print_metrics(trainer):\n",
    "    metrics = trainer.evaluate()\n",
    "    print(\"Available metrics:\")\n",
    "    for key in metrics.keys():\n",
    "        print(key)\n",
    "\n",
    "# После создания тренера и выполнения тренировки\n",
    "evaluate_and_print_metrics(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12328a77-6847-4cf3-a268-9ab9283d6649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
