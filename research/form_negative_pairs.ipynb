{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1ab662-38c8-4036-9444-48e2a2939c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, text_column):\n",
    "        self.texts = dataframe[text_column].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]#f\"query: {self.texts[idx]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9749f862-d454-4b77-bb2a-ae89a0c81b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db87f95-a8e1-4bbb-8b43-f049a722f8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Та пыгрисит маим вармаль э̄рнэ поратэт ат верм...</td>\n",
       "      <td>Те мальчики не выполнят задание в назначенный ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ха̄йтыматэ тӯр ва̄тан ёхтыс, вит ва̄тан ха̄йтыс.</td>\n",
       "      <td>Бегая к берегу озера пришла, к воде подбежала.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Вит са̄мыл сунсым о̄нтыс</td>\n",
       "      <td>Вода прибывала на глазах</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Атаявев, акваг лылынг тагл ворн та тотавев.</td>\n",
       "      <td>Обнюхивает нас, живыми на кладбище уносит.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ман ты пӣлтал, веськат хумиюв нэтхуньт ат ёрув...</td>\n",
       "      <td>Мы никогда не забудем этого честного человека.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79928</th>\n",
       "      <td>А̄нумн ка̄салахты аквтуп тамле о̄лнэ накыт ма̄...</td>\n",
       "      <td>Мне кажется, что подобные случаи могут вызыват...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79929</th>\n",
       "      <td>А̄танэ нё̄тнэ̄г юил акван-атманэ.</td>\n",
       "      <td>Волосы аккуратно собраны сзади.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79930</th>\n",
       "      <td>Тох тай, культура сака тэ̄пгалан мед а̄тим.</td>\n",
       "      <td>В общем, культуры интенсивного потребления мед...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79931</th>\n",
       "      <td>Тувыл Уэйтс ты музыкантыг ёт, Чарли Рич ос Фрэ...</td>\n",
       "      <td>Затем Уэйтс отправился на гастроли с такими му...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79932</th>\n",
       "      <td>Тох ман ро̄ви та̄ра юртанын воськасаӈквет?</td>\n",
       "      <td>Но разве можно вот так просто покинуть своих д...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79933 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  target  \\\n",
       "0      Та пыгрисит маим вармаль э̄рнэ поратэт ат верм...   \n",
       "1       Ха̄йтыматэ тӯр ва̄тан ёхтыс, вит ва̄тан ха̄йтыс.   \n",
       "2                               Вит са̄мыл сунсым о̄нтыс   \n",
       "3            Атаявев, акваг лылынг тагл ворн та тотавев.   \n",
       "4      Ман ты пӣлтал, веськат хумиюв нэтхуньт ат ёрув...   \n",
       "...                                                  ...   \n",
       "79928  А̄нумн ка̄салахты аквтуп тамле о̄лнэ накыт ма̄...   \n",
       "79929                  А̄танэ нё̄тнэ̄г юил акван-атманэ.   \n",
       "79930        Тох тай, культура сака тэ̄пгалан мед а̄тим.   \n",
       "79931  Тувыл Уэйтс ты музыкантыг ёт, Чарли Рич ос Фрэ...   \n",
       "79932         Тох ман ро̄ви та̄ра юртанын воськасаӈквет?   \n",
       "\n",
       "                                                  source  \n",
       "0      Те мальчики не выполнят задание в назначенный ...  \n",
       "1         Бегая к берегу озера пришла, к воде подбежала.  \n",
       "2                               Вода прибывала на глазах  \n",
       "3             Обнюхивает нас, живыми на кладбище уносит.  \n",
       "4         Мы никогда не забудем этого честного человека.  \n",
       "...                                                  ...  \n",
       "79928  Мне кажется, что подобные случаи могут вызыват...  \n",
       "79929                    Волосы аккуратно собраны сзади.  \n",
       "79930  В общем, культуры интенсивного потребления мед...  \n",
       "79931  Затем Уэйтс отправился на гастроли с такими му...  \n",
       "79932  Но разве можно вот так просто покинуть своих д...  \n",
       "\n",
       "[79933 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/cleared_v2/cleared_v2.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e5bb09c-ebb1-4338-8985-0f6c1ab2dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbcb07f7-df32-4055-8fc1-13f476a7a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(data, 'source')\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f718275-dd88-4583-96f2-1c032febe1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9bdbde9-fd72-4b80-b91c-03274473499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "# Each input text should start with \"query: \" or \"passage: \", even for non-English texts.\n",
    "# For tasks other than retrieval, you can simply use the \"query: \" prefix.\n",
    "input_texts = ['query: how much protein should a female eat',\n",
    "               'query: 南瓜的家常做法',\n",
    "               \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "               \"passage: 1.清炒南瓜丝 原料:嫩南瓜半个 调料:葱、盐、白糖、鸡精 做法: 1、南瓜用刀薄薄的削去表面一层皮,用勺子刮去瓤 2、擦成细丝(没有擦菜板就用刀慢慢切成细丝) 3、锅烧热放油,入葱花煸出香味 4、入南瓜丝快速翻炒一分钟左右,放盐、一点白糖和鸡精调味出锅 2.香葱炒南瓜 原料:南瓜1只 调料:香葱、蒜末、橄榄油、盐 做法: 1、将南瓜去皮,切成片 2、油锅8成热后,将蒜末放入爆香 3、爆香后,将南瓜片放入,翻炒 4、在翻炒的同时,可以不时地往锅里加水,但不要太多 5、放入盐,炒匀 6、南瓜差不多软和绵了之后,就可以关火 7、撒入香葱,即可出锅\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\n",
    "model = AutoModel.from_pretrained('intfloat/multilingual-e5-large')\n",
    "model = model.to('cuda')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3321d31-725d-44fd-8547-2f877d7811b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90.81391906738281, 72.1339111328125], [70.5354232788086, 88.76107788085938]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**{k: v.to('cuda') for k, v in batch_dict.items()})\n",
    "embeddings = average_pool(outputs.last_hidden_state.cpu(), batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab0870f-d5e8-4827-8b4d-64b8b107f334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003e10a6881c4c32a43961dd5446d2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader):\n",
    "        batch_dict = tokenizer(batch, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        batch_dict = {k: v.to('cuda') for k, v in batch_dict.items()}\n",
    "        \n",
    "        outputs = model(**batch_dict)\n",
    "        embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu().numpy().copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c6788-6326-4fcd-9a21-114b1ee9b0dd",
   "metadata": {},
   "source": [
    "# rubert-tiny-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f95bd48a-b498-4bbd-be8e-fb3ed80cfbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8918, 0.9049],\n",
      "        [0.8918, 1.0000, 0.8783],\n",
      "        [0.9049, 0.8783, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('sergeyzh/rubert-tiny-turbo', device='cuda')\n",
    "model.eval()\n",
    "\n",
    "sentences = [\"привет мир\", \"hello world\", \"здравствуй вселенная\"]\n",
    "embeddings = model.encode(sentences)\n",
    "print(util.dot_score(embeddings, embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898ef858-f3c6-4523-8ac2-df39c505d14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.6697],\n",
      "        [0.6697, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"вода лилась рекой\", \"камень стоял под столом\"]\n",
    "embeddings = model.encode(sentences)\n",
    "print(util.dot_score(embeddings, embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66047311-bb69-43a3-952c-9ee1d0ea3977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a8254b544040cab5f7a45b2111dff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_embeddings = []\n",
    "for batch in tqdm(loader):\n",
    "    all_embeddings.append(model.encode(batch).copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0257e537-8e1e-4de8-92aa-23d4d29ef42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171fc96-d3c4-4ec3-917e-09b225eb13af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f5b83e6-7ac8-4c9b-8618-68b4258e023e",
   "metadata": {},
   "source": [
    "# data from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e47b7fd-1f5c-43b8-8144-9d6595872ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1645286\\venvs\\py310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import h5py\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from ast import literal_eval\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba016efc-4b81-4b35-ae8a-5af7872b17f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Та пыгрисит маим вармаль э̄рнэ поратэт ат вермгыг варункв</td>\n",
       "      <td>Те мальчики не выполнят задание в назначенный срок.</td>\n",
       "      <td>{0: 1.0, 52122: 0.9042518734931946, 18930: 0.8891035318374634, 41202: 0.8863261342048645, 6959: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ха̄йтыматэ тӯр ва̄тан ёхтыс, вит ва̄тан ха̄йтыс.</td>\n",
       "      <td>Бегая к берегу озера пришла, к воде подбежала.</td>\n",
       "      <td>{1: 0.9999999403953552, 4463: 0.9709538817405701, 40149: 0.9473190307617188, 10594: 0.9426226615...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Вит са̄мыл сунсым о̄нтыс</td>\n",
       "      <td>Вода прибывала на глазах</td>\n",
       "      <td>{2: 0.9999999403953552, 33249: 0.9213534593582153, 11802: 0.9161194562911987, 27806: 0.907817661...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Атаявев, акваг лылынг тагл ворн та тотавев.</td>\n",
       "      <td>Обнюхивает нас, живыми на кладбище уносит.</td>\n",
       "      <td>{3: 0.9999997019767761, 12914: 0.9601884484291077, 4014: 0.9214043021202087, 62795: 0.9111366868...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ман ты пӣлтал, веськат хумиюв нэтхуньт ат ёрувлвылув</td>\n",
       "      <td>Мы никогда не забудем этого честного человека.</td>\n",
       "      <td>{4: 1.0, 55368: 0.9368814826011658, 61451: 0.9308530688285828, 70427: 0.9243783950805664, 58062:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77369</th>\n",
       "      <td>А̄нумн ка̄салахты аквтуп тамле о̄лнэ накыт ма̄хматын ма̄гыс лю̄льсаӈыг сусхатуӈкве ве̄рме̄гыт.</td>\n",
       "      <td>Мне кажется, что подобные случаи могут вызывать подозрение.</td>\n",
       "      <td>{77369: 1.0000001192092896, 75988: 0.9060379862785339, 75732: 0.8967535495758057, 40617: 0.89106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77370</th>\n",
       "      <td>А̄танэ нё̄тнэ̄г юил акван-атманэ.</td>\n",
       "      <td>Волосы аккуратно собраны сзади.</td>\n",
       "      <td>{77370: 1.0000001192092896, 25801: 0.9226779341697693, 25023: 0.9087227582931519, 48245: 0.90375...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77371</th>\n",
       "      <td>Тох тай, культура сака тэ̄пгалан мед а̄тим.</td>\n",
       "      <td>В общем, культуры интенсивного потребления меда нет.</td>\n",
       "      <td>{77371: 0.9999998211860657, 26336: 0.8946378231048584, 26326: 0.8911178112030029, 48607: 0.89045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77372</th>\n",
       "      <td>Тувыл Уэйтс ты музыкантыг ёт, Чарли Рич ос Фрэнк Заппа ёт гастролин минас.</td>\n",
       "      <td>Затем Уэйтс отправился на гастроли с такими музыкантами, как Чарли Рич и Фрэнк Заппа.</td>\n",
       "      <td>{77372: 0.9999999403953552, 31706: 0.8608967065811157, 53618: 0.8565278053283691, 66384: 0.85216...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77373</th>\n",
       "      <td>Тох ман ро̄ви та̄ра юртанын воськасаӈквет?</td>\n",
       "      <td>Но разве можно вот так просто покинуть своих друзей?</td>\n",
       "      <td>{77373: 0.9999999403953552, 18698: 0.8895999789237976, 10582: 0.8820904493331909, 45889: 0.87994...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77374 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                               target  \\\n",
       "0                                           Та пыгрисит маим вармаль э̄рнэ поратэт ат вермгыг варункв   \n",
       "1                                                   Ха̄йтыматэ тӯр ва̄тан ёхтыс, вит ва̄тан ха̄йтыс.   \n",
       "2                                                                            Вит са̄мыл сунсым о̄нтыс   \n",
       "3                                                         Атаявев, акваг лылынг тагл ворн та тотавев.   \n",
       "4                                               Ман ты пӣлтал, веськат хумиюв нэтхуньт ат ёрувлвылув   \n",
       "...                                                                                               ...   \n",
       "77369  А̄нумн ка̄салахты аквтуп тамле о̄лнэ накыт ма̄хматын ма̄гыс лю̄льсаӈыг сусхатуӈкве ве̄рме̄гыт.   \n",
       "77370                                                               А̄танэ нё̄тнэ̄г юил акван-атманэ.   \n",
       "77371                                                     Тох тай, культура сака тэ̄пгалан мед а̄тим.   \n",
       "77372                      Тувыл Уэйтс ты музыкантыг ёт, Чарли Рич ос Фрэнк Заппа ёт гастролин минас.   \n",
       "77373                                                      Тох ман ро̄ви та̄ра юртанын воськасаӈквет?   \n",
       "\n",
       "                                                                                      source  \\\n",
       "0                                        Те мальчики не выполнят задание в назначенный срок.   \n",
       "1                                             Бегая к берегу озера пришла, к воде подбежала.   \n",
       "2                                                                   Вода прибывала на глазах   \n",
       "3                                                 Обнюхивает нас, живыми на кладбище уносит.   \n",
       "4                                             Мы никогда не забудем этого честного человека.   \n",
       "...                                                                                      ...   \n",
       "77369                            Мне кажется, что подобные случаи могут вызывать подозрение.   \n",
       "77370                                                        Волосы аккуратно собраны сзади.   \n",
       "77371                                   В общем, культуры интенсивного потребления меда нет.   \n",
       "77372  Затем Уэйтс отправился на гастроли с такими музыкантами, как Чарли Рич и Фрэнк Заппа.   \n",
       "77373                                   Но разве можно вот так просто покинуть своих друзей?   \n",
       "\n",
       "                                                                                              similarities  \n",
       "0      {0: 1.0, 52122: 0.9042518734931946, 18930: 0.8891035318374634, 41202: 0.8863261342048645, 6959: ...  \n",
       "1      {1: 0.9999999403953552, 4463: 0.9709538817405701, 40149: 0.9473190307617188, 10594: 0.9426226615...  \n",
       "2      {2: 0.9999999403953552, 33249: 0.9213534593582153, 11802: 0.9161194562911987, 27806: 0.907817661...  \n",
       "3      {3: 0.9999997019767761, 12914: 0.9601884484291077, 4014: 0.9214043021202087, 62795: 0.9111366868...  \n",
       "4      {4: 1.0, 55368: 0.9368814826011658, 61451: 0.9308530688285828, 70427: 0.9243783950805664, 58062:...  \n",
       "...                                                                                                    ...  \n",
       "77369  {77369: 1.0000001192092896, 75988: 0.9060379862785339, 75732: 0.8967535495758057, 40617: 0.89106...  \n",
       "77370  {77370: 1.0000001192092896, 25801: 0.9226779341697693, 25023: 0.9087227582931519, 48245: 0.90375...  \n",
       "77371  {77371: 0.9999998211860657, 26336: 0.8946378231048584, 26326: 0.8911178112030029, 48607: 0.89045...  \n",
       "77372  {77372: 0.9999999403953552, 31706: 0.8608967065811157, 53618: 0.8565278053283691, 66384: 0.85216...  \n",
       "77373  {77373: 0.9999999403953552, 18698: 0.8895999789237976, 10582: 0.8820904493331909, 45889: 0.87994...  \n",
       "\n",
       "[77374 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('../data/data_with_scores.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca69ecb-ad25-4993-a2f0-10eb41367e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[В июне в этнографическом музее «Торум Маа» города Ханты-Мансийска дети посещали лагерь «Таксар ...</td>\n",
       "      <td>[Лӯпта э̄тпост Ханты-Мансийск ӯст «То̄рум Маа» музейт на̄врамыт ат хо̄тал ханищтахтасыт. Ты тэ...</td>\n",
       "      <td>https://khanty-yasang.ru//luima-seripos/no-14-1296/16763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Демид Чамалтынов, 9 лет, – Я живу в Ханты-Мансийске, обучаюсь в школе № 2. В этом году я впервы...</td>\n",
       "      <td>[Демид Чамалтынов, 9 та̄л, – «Та̄ксар ма̄хум» – «Крепкие люди» сменан ам ты та̄л о̄выл щёс ёхтыс...</td>\n",
       "      <td>https://khanty-yasang.ru//luima-seripos/no-16-1298/16923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[В Берёзовском районе в семи километрах ниже от посёлка Сосьва и в пятнадцати километрах от Усть...</td>\n",
       "      <td>[Ха̄льӯс районт Кульпас па̄вылныл ло̄ӈхаль са̄т ве̄рста Та̄гт а̄ хосыт ма̄нь ма̄ньщи па̄выл – С...</td>\n",
       "      <td>https://khanty-yasang.ru//luima-seripos/no-17-1275/15058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Я хочу написать про своих детей. Этот год для нас был очень важным. Мои дети Коля и Надя –два в...</td>\n",
       "      <td>[Ам амки на̄врама̄гум урыл хансуӈкв таӈхе̄гум. Ты та̄л ам ма̄гсылум тамле о̄лыс, ам кит на̄врама...</td>\n",
       "      <td>https://khanty-yasang.ru//luima-seripos/no-13-1295/16691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Каждое лето Ханты-Мансийский этнографический музей под открытым небом «Торум Маа» для маленьких...</td>\n",
       "      <td>[Ка̄сыӈ туи пора Ханты-Мансийск ӯст о̄лнэ на̄врамыт магыс «То̄рум Маа» музеит ханищтап ва̄раве,...</td>\n",
       "      <td>https://khanty-yasang.ru//luima-seripos/no-17-1299/17003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>[Многие молодые ребята из числа коренных малочисленных народов Севера после окончания школы охот...</td>\n",
       "      <td>[Ханты-Мансийск ӯст акв ма̄ньщи пыг о̄лы. Аквматнакт ам тав уре̄т хансыгла̄лсум. Тав наме Алекс...</td>\n",
       "      <td>https://khanty-yasang.ru//luima-seripos/no-4-1214/10040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>[С хантыйской мастерицей Надеждой Алексеевной Гришкиной я познакомилась в музее «Торум Маа», где...</td>\n",
       "      <td>[Надежда Алексеевна Гришкина ты та̄л атпан нупыл ат та̄лэ то̄влыс. Нэ̄ ще̄мьятэ ёт Тугияны па̄вы...</td>\n",
       "      <td>https://khanty-yasang.ru//luima-seripos/no-24-1186/8519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>[Для жителей сельского поселения села Саранпауль традиционный праздник «День оленевода» - один и...</td>\n",
       "      <td>[Ма̄н округувт са̄в са̄лыт туп тит ма̄т о̄ньщавет, ты Ха̄льӯс район Саранпа̄вылт ос Белоярский ...</td>\n",
       "      <td>https://khanty-yasang.ru//luima-seripos/no-5-1167/7614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>[Альбина Волосовская уже много лет живет и работает в п. Октябрьском. Сама она родом из маленько...</td>\n",
       "      <td>[Ты хурит Октябрьский па̄вылт о̄лнэ ма̄ньщи нэ̄ Альбина Петровна Волосовская по̄слым о̄лы. Такви...</td>\n",
       "      <td>https://khanty-yasang.ru//luima-seripos/no-14-1056/896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>[В Ханты-Мансийске в июне месяце состоялись «III Шесталовские чтения», куда с разных территорий ...</td>\n",
       "      <td>[Ха̄льӯст ма̄ньлат ма̄ньщи хум Михаил Яркин о̄лы. Тав «Центр безопасности» нампа колт рӯпиты, ...</td>\n",
       "      <td>https://khanty-yasang.ru//luima-seripos/no-13-1271/14700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3750 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   source  \\\n",
       "0     [В июне в этнографическом музее «Торум Маа» города Ханты-Мансийска дети посещали лагерь «Таксар ...   \n",
       "1     [Демид Чамалтынов, 9 лет, – Я живу в Ханты-Мансийске, обучаюсь в школе № 2. В этом году я впервы...   \n",
       "2     [В Берёзовском районе в семи километрах ниже от посёлка Сосьва и в пятнадцати километрах от Усть...   \n",
       "3     [Я хочу написать про своих детей. Этот год для нас был очень важным. Мои дети Коля и Надя –два в...   \n",
       "4     [Каждое лето Ханты-Мансийский этнографический музей под открытым небом «Торум Маа» для маленьких...   \n",
       "...                                                                                                   ...   \n",
       "3745  [Многие молодые ребята из числа коренных малочисленных народов Севера после окончания школы охот...   \n",
       "3746  [С хантыйской мастерицей Надеждой Алексеевной Гришкиной я познакомилась в музее «Торум Маа», где...   \n",
       "3747  [Для жителей сельского поселения села Саранпауль традиционный праздник «День оленевода» - один и...   \n",
       "3748  [Альбина Волосовская уже много лет живет и работает в п. Октябрьском. Сама она родом из маленько...   \n",
       "3749  [В Ханты-Мансийске в июне месяце состоялись «III Шесталовские чтения», куда с разных территорий ...   \n",
       "\n",
       "                                                                                                   target  \\\n",
       "0     [Лӯпта э̄тпост Ханты-Мансийск ӯст «То̄рум Маа» музейт на̄врамыт ат хо̄тал ханищтахтасыт. Ты тэ...   \n",
       "1     [Демид Чамалтынов, 9 та̄л, – «Та̄ксар ма̄хум» – «Крепкие люди» сменан ам ты та̄л о̄выл щёс ёхтыс...   \n",
       "2     [Ха̄льӯс районт Кульпас па̄вылныл ло̄ӈхаль са̄т ве̄рста Та̄гт а̄ хосыт ма̄нь ма̄ньщи па̄выл – С...   \n",
       "3     [Ам амки на̄врама̄гум урыл хансуӈкв таӈхе̄гум. Ты та̄л ам ма̄гсылум тамле о̄лыс, ам кит на̄врама...   \n",
       "4     [Ка̄сыӈ туи пора Ханты-Мансийск ӯст о̄лнэ на̄врамыт магыс «То̄рум Маа» музеит ханищтап ва̄раве,...   \n",
       "...                                                                                                   ...   \n",
       "3745  [Ханты-Мансийск ӯст акв ма̄ньщи пыг о̄лы. Аквматнакт ам тав уре̄т хансыгла̄лсум. Тав наме Алекс...   \n",
       "3746  [Надежда Алексеевна Гришкина ты та̄л атпан нупыл ат та̄лэ то̄влыс. Нэ̄ ще̄мьятэ ёт Тугияны па̄вы...   \n",
       "3747  [Ма̄н округувт са̄в са̄лыт туп тит ма̄т о̄ньщавет, ты Ха̄льӯс район Саранпа̄вылт ос Белоярский ...   \n",
       "3748  [Ты хурит Октябрьский па̄вылт о̄лнэ ма̄ньщи нэ̄ Альбина Петровна Волосовская по̄слым о̄лы. Такви...   \n",
       "3749  [Ха̄льӯст ма̄ньлат ма̄ньщи хум Михаил Яркин о̄лы. Тав «Центр безопасности» нампа колт рӯпиты, ...   \n",
       "\n",
       "                                                          link  \n",
       "0     https://khanty-yasang.ru//luima-seripos/no-14-1296/16763  \n",
       "1     https://khanty-yasang.ru//luima-seripos/no-16-1298/16923  \n",
       "2     https://khanty-yasang.ru//luima-seripos/no-17-1275/15058  \n",
       "3     https://khanty-yasang.ru//luima-seripos/no-13-1295/16691  \n",
       "4     https://khanty-yasang.ru//luima-seripos/no-17-1299/17003  \n",
       "...                                                        ...  \n",
       "3745   https://khanty-yasang.ru//luima-seripos/no-4-1214/10040  \n",
       "3746   https://khanty-yasang.ru//luima-seripos/no-24-1186/8519  \n",
       "3747    https://khanty-yasang.ru//luima-seripos/no-5-1167/7614  \n",
       "3748    https://khanty-yasang.ru//luima-seripos/no-14-1056/896  \n",
       "3749  https://khanty-yasang.ru//luima-seripos/no-13-1271/14700  \n",
       "\n",
       "[3750 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_corpus = pd.read_pickle('../data/parsed_corpus_changed.pkl')\n",
    "parsed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8dc3e8f-7a32-4ce6-a1c5-d3dd7e4b4e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cffb5f50-fa06-4a10-8ec9-38281588b711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1645286\\venvs\\py310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02882476 -0.00602382 -0.05947006 ... -0.0300225  -0.02960701\n",
      "   0.00067479]\n",
      " [-0.05550231  0.02546487 -0.02157258 ...  0.02932104  0.01150043\n",
      "  -0.00848787]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1645286\\venvs\\py310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "labse = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "embeddings = labse.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e146738b-22d2-4a44-8c2d-716eec6d53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpus = data.target.tolist() + parsed_corpus.target.apply(lambda x: '\\n'.join(x)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b21f64-34ed-4596-8de1-b5510a872eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed55b7-2dac-47f6-8b36-f24633c9dfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6a5f1-a2f0-4363-8a97-1cbe6f0e1324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d7965a5-c859-4f8a-b72f-1e5c8625bc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Бегая к берегу озера пришла, к воде подбежала.',\n",
       " 'Ха̄йтыматэ тӯр ва̄тан ёхтыс, вит ва̄тан ха̄йтыс.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.source.loc[1], data.target.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42fb142-cb7a-4a56-bee7-1b8987ef5eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Ха', '##̄', '##йт', '##ыма', '##тэ', 'ту', '##̄', '##р', 'ва', '##̄', '##тан', 'ё', '##хт', '##ыс', ',', 'ви', '##т', 'ва', '##̄', '##тан', 'ха', '##̄', '##йт', '##ыс', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(labse.tokenizer.convert_ids_to_tokens(labse.tokenizer.encode(data.target.loc[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a60ec779-892d-4bc8-9974-ea26693f550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Бе', '##га', '##я', 'к', 'берегу', 'озера', 'пришла', ',', 'к', 'воде', 'под', '##бежал', '##а', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(labse.tokenizer.convert_ids_to_tokens(labse.tokenizer.encode(data.source.loc[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a63df1b-c1ba-4818-b226-1be3da16c1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501153"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labse.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b7e379-7404-49b8-93d1-6676126cfaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='sentence-transformers/LaBSE', vocab_size=501153, model_max_length=256, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokenizer = deepcopy(labse.tokenizer)\n",
    "old_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa5734-9d3a-4e04-9db4-c1b8ecb0b8c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## add new tokens to tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a87f9a2f-18e4-4fbf-a59b-437dfee1b5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134842264b50412ab011b991ee446da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_freqs = Counter()\n",
    "\n",
    "for text in tqdm(all_corpus):\n",
    "    words_with_offsets = labse.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        if len(word) > 1:\n",
    "            word_freqs[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b85d93d-d74d-4a24-a70c-71e713102167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ос', 38228),\n",
       " ('ты', 17358),\n",
       " ('та', 14719),\n",
       " ('ат', 14211),\n",
       " ('ма̄хум', 13972),\n",
       " ('Ты', 12616),\n",
       " ('тав', 11556),\n",
       " ('та̄л', 10314),\n",
       " ('о̄лнэ', 9334),\n",
       " ('урыл', 9088),\n",
       " ('о̄с', 9064),\n",
       " ('са̄в', 8994),\n",
       " ('порат', 8686),\n",
       " ('ань', 8419),\n",
       " ('тот', 8407),\n",
       " ('та̄н', 8273),\n",
       " ('ма̄н', 8049),\n",
       " ('хо̄тпат', 7991),\n",
       " ('ёт', 7825),\n",
       " ('Ань', 7633)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "531b298c-cc83-4960-9899-363521c8f2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¬', '¸', '×', 'ä', 'é', 'ó', 'ö', 'ü', '̄', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ѐ', 'ё', 'є', 'ў', 'ӆ', 'Ӈ', 'ӈ', 'ӊ', 'ӑ', 'ә', '№', '™']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "803d0804-3e52-4319-b6d7-4599cec30022",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = deepcopy(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa0076c1-f93e-496b-aada-c363e7b03332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Та': ['Т', 'а'],\n",
       " 'пыгрисит': ['п', 'ы', 'г', 'р', 'и', 'с', 'и', 'т'],\n",
       " 'маим': ['м', 'а', 'и', 'м'],\n",
       " 'вармаль': ['в', 'а', 'р', 'м', 'а', 'л', 'ь'],\n",
       " 'э̄рнэ': ['э', '̄', 'р', 'н', 'э']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "dict(zip(\n",
    "    list(init_splits.keys())[:5],\n",
    "    list(init_splits.values())[:5]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d11e3437-21ac-4391-a253-7fc8fe8a5c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits, word_freqs, disable_tqdm=False):\n",
    "    pair_freqs = Counter()\n",
    "    for word, freq in tqdm(word_freqs.items(), disable=disable_tqdm):\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89750909-85cd-47e7-bcf3-835fa6192038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566755d4b6454b21a3b70b535c671050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_pair_freqs = compute_pair_freqs(init_splits, word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0775045a-3f1f-4424-b7b5-63765ed51ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('а', '̄'), 354565),\n",
       " (('т', 'а'), 231852),\n",
       " (('о', '̄'), 173997),\n",
       " (('т', 'ы'), 153771),\n",
       " (('а', 'н'), 149215),\n",
       " (('м', 'а'), 147474),\n",
       " (('ы', 'л'), 140337),\n",
       " (('ы', 'т'), 132754),\n",
       " (('̄', 'л'), 130946),\n",
       " (('̄', 'н'), 121158)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_pair_freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "254aef2d-bada-4a11-a2a2-533945fbab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits, word_freqs, disable_tqdm=False):\n",
    "    for word in tqdm(word_freqs, disable=disable_tqdm):\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "798fe9f0-9603-41b7-bb37-e795bb6b205b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8eb8fa7e084befb3cc8e02176d9370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4853 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "merges = {}\n",
    "old_vocab_size = len(vocab)\n",
    "\n",
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "\n",
    "for _ in tqdm(range(vocab_size - old_vocab_size)):\n",
    "    pair_freqs = compute_pair_freqs(splits, word_freqs, True)\n",
    "    best_pair, _ = pair_freqs.most_common(1)[0]\n",
    "    splits = merge_pair(*best_pair, splits, word_freqs, True)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73a0aedc-0cfa-412e-88b4-71e68784b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('merges.pkl', 'wb') as f:\n",
    "    pickle.dump(merges, f)\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a3d436a-4345-42d9-89dc-ab6422c1c0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9bf3ba9-6171-48be-ac41-da1454b5e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = deepcopy(old_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c7275da-bf15-4f56-8334-4f21b4d1be13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ӯсн', 'лё̄', 'тамле', 'ма̄ныл', 'са̄вит']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = list(set(vocab[:1000]) - set(new_tokenizer.vocab.keys()))\n",
    "new_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8167c3b4-350f-4820-b554-c5f4ae3cd37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14e80042-0c7b-4243-b122-844360714acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2e1a8f48-810c-4520-8dab-e6d24be369c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ха̄йтыматэ тӯр ва̄тан ёхтыс, вит ва̄тан ха̄йтыс.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af97f4cc-dc41-485a-bb47-eb2acfcfe7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Ха̄', 'йт', 'ы', '##м', 'атэ', 'тӯ', 'р', 'ва̄т', 'ан', 'ёхтыс', ',', 'вит', 'ва̄т', 'ан', 'ха̄й', 'т', 'ыс', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer.convert_ids_to_tokens(new_tokenizer.encode(data.target.loc[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bfd26a94-754e-467a-b876-64c1295791a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Бе', '##га', '##я', 'к', 'берегу', 'озера', 'пришла', ',', 'к', 'воде', 'под', '##бежал', '##а', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer.convert_ids_to_tokens(new_tokenizer.encode(data.source.loc[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3cf65-6887-4f9b-bf93-c9a05e308bf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# modified tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c1e2a7ee-157f-4d5c-ad4b-bd16b73c8cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='sentence-transformers/LaBSE', vocab_size=501153, model_max_length=256, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokenizer = deepcopy(labse.tokenizer)\n",
    "old_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880941c6-ee3e-4ce4-918c-1c25887034f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## bpe train (not correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "150c9956-0729-4b6f-94e9-1a1de174c125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ос', 38228),\n",
       " ('ты', 17358),\n",
       " ('та', 14719),\n",
       " ('ат', 14211),\n",
       " ('ма̄хум', 13972),\n",
       " ('Ты', 12616),\n",
       " ('тав', 11556),\n",
       " ('та̄л', 10314),\n",
       " ('о̄лнэ', 9334),\n",
       " ('урыл', 9088)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55e20cde-edf5-4d1e-a423-ff79ce12fb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36124ada8e9a4664bb6dade986408389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##0', '##1', '##2', '##3', '##4', '##5', '##6', '##7', '##8', '##9', '##A', '##B', '##C', '##D', '##E', '##F', '##G', '##H', '##I', '##J', '##K', '##L', '##M', '##N', '##O', '##P', '##R', '##S', '##T', '##U', '##V', '##W', '##X', '##Y', '##Z', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##q', '##r', '##s', '##t', '##u', '##v', '##w', '##x', '##y', '##z', '##¬', '##¸', '##×', '##ä', '##é', '##ó', '##ö', '##ü', '##̄', '##Ё', '##А', '##Б', '##В', '##Г', '##Д', '##Е', '##Ж', '##З', '##И', '##Й', '##К', '##Л', '##М', '##Н', '##О', '##П', '##Р', '##С', '##Т', '##У', '##Ф', '##Х', '##Ц', '##Ч', '##Ш', '##Щ', '##Ы', '##Ь', '##Э', '##Ю', '##Я', '##а', '##б', '##в', '##г', '##д', '##е', '##ж', '##з', '##и', '##й', '##к', '##л', '##м', '##н', '##о', '##п', '##р', '##с', '##т', '##у', '##ф', '##х', '##ц', '##ч', '##ш', '##щ', '##ъ', '##ы', '##ь', '##э', '##ю', '##я', '##ѐ', '##ё', '##є', '##ў', '##ӆ', '##Ӈ', '##ӈ', '##ӊ', '##ӑ', '##ә', '##™', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '̄', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ы', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ѐ', 'ё', 'ӈ', 'ӑ', '№']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in tqdm(word_freqs.keys()):\n",
    "    for i, letter in enumerate(word):\n",
    "        if i == 0:\n",
    "            if letter not in alphabet:\n",
    "                alphabet.append(letter)\n",
    "        else:\n",
    "            if '##' + letter not in alphabet:\n",
    "                alphabet.append('##' + letter)\n",
    "                \n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5884968b-2f52-4a87-a8e2-f4f01d56cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = deepcopy(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ad0e3b4d-30f7-4f15-9457-ed96c8cec51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d4dd1fb7b5455cb558f0bedf081a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_splits_v2(word_freqs, disable_tqdm=False):\n",
    "    splits = {}\n",
    "    for word in tqdm(word_freqs, disable=disable_tqdm):\n",
    "        l = ['##' + c for c in word]\n",
    "        l[0] = word[0]\n",
    "        splits[word] = deepcopy(l)\n",
    "    return splits\n",
    "splits = get_splits_v2(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0d26d541-9f09-4da0-a0f2-b4695e481807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Та': ['Т', '##а'],\n",
       " 'пыгрисит': ['п', '##ы', '##г', '##р', '##и', '##с', '##и', '##т'],\n",
       " 'маим': ['м', '##а', '##и', '##м'],\n",
       " 'вармаль': ['в', '##а', '##р', '##м', '##а', '##л', '##ь'],\n",
       " 'э̄рнэ': ['э', '##̄', '##р', '##н', '##э']}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(\n",
    "    list(splits.keys())[:5],\n",
    "    list(splits.values())[:5]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2897aeb2-f17e-4187-a9d5-9b48e97c33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs_v2(splits, word_freqs, disable_tqdm=False):\n",
    "    pair_freqs = Counter()\n",
    "    for word, freq in tqdm(word_freqs.items(), disable=disable_tqdm):\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        for i in range(len(split) - 2):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "46fd3295-5f14-45e4-93df-0c7ec276cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair_v2(a, b, splits, word_freqs, disable_tqdm=False):\n",
    "    for word in tqdm(word_freqs, disable=disable_tqdm):\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                if i == 0:\n",
    "                    split = split[:i] + [a.replace('##', '') + b.replace('##', '')] + split[i + 2:]\n",
    "                else:\n",
    "                    split = split[:i] + ['##' + a.replace('##', '') + b.replace('##', '')] + split[i + 2:]\n",
    "            else:\n",
    "                i += 1\n",
    "                \n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d2c40647-ddcb-43f2-8414-b93fbbecee38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202024dc95f14980a073397607cbe5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "additional_vocab_size = 100\n",
    "merges = {}\n",
    "\n",
    "splits = get_splits_v2(word_freqs, True)\n",
    "\n",
    "for _ in tqdm(range(additional_vocab_size)):\n",
    "    pair_freqs = compute_pair_freqs_v2(splits, word_freqs, True)\n",
    "    best_pair, _ = pair_freqs.most_common(1)[0]\n",
    "    splits = merge_pair_v2(*best_pair, splits, word_freqs, True)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25b814-5f58-4890-ba19-f8fb356197a1",
   "metadata": {},
   "source": [
    "## wordpiece training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6a76671b-9105-415f-b462-a130ea8fec40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ос', 38228),\n",
       " ('ты', 17358),\n",
       " ('та', 14719),\n",
       " ('ат', 14211),\n",
       " ('ма̄хум', 13972),\n",
       " ('Ты', 12616),\n",
       " ('тав', 11556),\n",
       " ('та̄л', 10314),\n",
       " ('о̄лнэ', 9334),\n",
       " ('урыл', 9088)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3bab63b1-722a-4a00-b6c1-7b453a32294e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c9d64f9e844070a44c2d549b04275e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 256). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33371"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([text for text in tqdm(all_corpus) if old_tokenizer.unk_token_id in old_tokenizer.encode(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a17de-084f-4632-a0b3-e125b73582aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a4676-1612-4f90-91bb-836e868b56c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628ad02-754a-47ac-a239-adacbf3b6579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d3e544f8-40b5-4f12-8add-00b18811331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tokens(vocab):\n",
    "    new_tokenizer = deepcopy(labse.tokenizer)\n",
    "    new_tokens = list(set(vocab) - set(new_tokenizer.vocab.keys()))\n",
    "    num_tokens_added = new_tokenizer.add_tokens(new_tokens)\n",
    "    print(f'Added {num_tokens_added} new tokens to tokenizer')\n",
    "    return new_tokenizer, new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2fc346f5-7451-474a-923b-28c8c6e03465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ханищ##та']\n",
      "['ханищта']\n",
      "Added 1 new tokens to tokenizer\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer, new_tokens = add_tokens(['ханищ##та'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2c6c391d-f6f7-4deb-8e7e-4fe97577384d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ханищта': 501153}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in new_tokenizer.vocab.items() if 'ханищта' in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fe18076-0239-436d-b8e0-9d32f8254a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import models, pre_tokenizers, decoders, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce411f2c-7788-45a6-a356-bd337c33ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wordpiece = Tokenizer(models.WordPiece())\n",
    "new_wordpiece.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "#new_wordpiece.decoder = decoders.ByteLevel()\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=1500,\n",
    "    min_frequency=30,\n",
    "    show_progress=True,\n",
    "    continuing_subword_prefix='##'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c8788d38-fb10-48c7-81e8-4ea346ebe665",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wordpiece.train_from_iterator(all_corpus, trainer=trainer, length=len(all_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d8b2cf6-9e20-4886-b08f-6debfedfa10b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ханищтахтуӈкве',\n",
       " 'рӯпитаӈкве',\n",
       " 'э̄лумхо̄лас',\n",
       " 'рӯпитэ̄гыт',\n",
       " 'ханищтахтын',\n",
       " 'потыртаӈкве',\n",
       " 'ханищтахтас',\n",
       " '##лумхо̄лас',\n",
       " 'Саранпа̄выл',\n",
       " 'хо̄нтхатыг',\n",
       " 'о̄ньще̄гыт',\n",
       " 'ве̄рме̄гыт',\n",
       " '##таве̄сыт',\n",
       " '##хатэ̄гыт',\n",
       " 'потыртасыт',\n",
       " '##лаве̄сыт',\n",
       " '##има̄гыс',\n",
       " 'та̄рвитыӈ',\n",
       " 'ва̄руӈкве',\n",
       " 'ва̄ре̄гыт',\n",
       " 'ва̄рмалит',\n",
       " '##щхӣпыӈ',\n",
       " 'таима̄гыс',\n",
       " 'та̄нанылн',\n",
       " 'па̄вылныл',\n",
       " 'ня̄врамыт',\n",
       " '##хуйплов',\n",
       " 'ёхталасыт',\n",
       " 'ма̄хманув',\n",
       " 'Александр',\n",
       " 'суссылтап',\n",
       " 'на̄врамыт',\n",
       " 'хансуӈкве',\n",
       " 'хо̄талэ̄т',\n",
       " 'рӯпитас',\n",
       " 'ва̄рмаль',\n",
       " '##луӈкве',\n",
       " 'пормасыт',\n",
       " 'са̄ӈквыл',\n",
       " 'ханищтап',\n",
       " '##а̄врам',\n",
       " 'хо̄талыт',\n",
       " 'о̄лэ̄гыт',\n",
       " '##лаӈкве',\n",
       " 'Светлана',\n",
       " '##та̄гыл',\n",
       " '##туӈкве',\n",
       " 'ма̄ньлат',\n",
       " '##тэ̄гыт',\n",
       " '##е̄ккар',\n",
       " '##ве̄сыт',\n",
       " 'о̄вылтах',\n",
       " 'нэ̄пакыт',\n",
       " 'Владимир',\n",
       " 'Ха̄льӯс',\n",
       " '##лэ̄гыт',\n",
       " '##тыглас',\n",
       " '##па̄выл',\n",
       " 'пӯльниц',\n",
       " 'Мансийск',\n",
       " '##хо̄тал',\n",
       " 'та̄наныл',\n",
       " 'солко̄ви',\n",
       " '##тыглах',\n",
       " '##таӈкве',\n",
       " 'рӯпитан',\n",
       " 'ищхӣпыӈ',\n",
       " 'ханищтах',\n",
       " 'ханищтан',\n",
       " 'потыртас',\n",
       " 'ӯргалан',\n",
       " 'о̄вылтыт',\n",
       " 'округувт',\n",
       " 'халанылт',\n",
       " 'патэ̄гыт',\n",
       " '##ве̄сум',\n",
       " '##тасы̄г',\n",
       " '##тыяныл',\n",
       " '##хо̄лас',\n",
       " 'о̄луӈкве',\n",
       " '##партам',\n",
       " '##тэ̄гум',\n",
       " '##лавес',\n",
       " 'Алексан',\n",
       " '##тастэ',\n",
       " 'рӯпата',\n",
       " '##хатас',\n",
       " 'па̄вылт',\n",
       " 'са̄ккон',\n",
       " 'яныгмас',\n",
       " 'ёмащакв',\n",
       " 'коныпал',\n",
       " 'ма̄щтыр',\n",
       " '##е̄гум',\n",
       " 'ла̄тӈыл',\n",
       " 'кӯщаиг',\n",
       " 'рӯпиты',\n",
       " 'па̄влыт',\n",
       " 'ле̄ккар',\n",
       " 'хультум',\n",
       " '##а̄вит',\n",
       " '##̄врам',\n",
       " '##э̄гын',\n",
       " '##ласыт',\n",
       " 'Ма̄ньщи',\n",
       " 'ла̄ххал',\n",
       " '##хатыг',\n",
       " 'На̄врам',\n",
       " '##льниц',\n",
       " '##янӯв',\n",
       " '##еский',\n",
       " '##а̄выл',\n",
       " '##рищит',\n",
       " 'хо̄талт',\n",
       " '##таӈкв',\n",
       " 'атхатыг',\n",
       " 'мӯйлуп',\n",
       " '##тавес',\n",
       " '##овинь',\n",
       " '##саныл',\n",
       " '##тасыт',\n",
       " '##ласум',\n",
       " 'то̄влыс',\n",
       " 'янытлан',\n",
       " '##ӈквыл',\n",
       " '##э̄гыт',\n",
       " 'па̄сныл',\n",
       " 'па̄вылн',\n",
       " '##тыгла',\n",
       " 'хо̄тпаг',\n",
       " 'э̄тпост',\n",
       " '##е̄гыт',\n",
       " 'са̄всыр',\n",
       " '##има̄г',\n",
       " '##лавет',\n",
       " 'Людмила',\n",
       " '##тавет',\n",
       " 'ма̄хумн',\n",
       " 'Светлан',\n",
       " 'ще̄мьят',\n",
       " '##ернат',\n",
       " 'ма̄ньщи',\n",
       " '##па̄ла',\n",
       " '##матыр',\n",
       " '##о̄рум',\n",
       " 'ма̄гсыл',\n",
       " 'ва̄рмал',\n",
       " '##ргани',\n",
       " 'ня̄врам',\n",
       " 'хо̄тпат',\n",
       " 'Татьяна',\n",
       " 'Николай',\n",
       " '##а̄гыл',\n",
       " '##э̄гум',\n",
       " '##ияныл',\n",
       " 'со̄тыра',\n",
       " 'Кульпас',\n",
       " '##уӈкве',\n",
       " '##тасум',\n",
       " '##дмила',\n",
       " 'тотыгла',\n",
       " 'на̄врам',\n",
       " '##хатым',\n",
       " 'кӯщаит',\n",
       " 'Ха̄льӯ',\n",
       " 'ла̄ваве',\n",
       " '##анылт',\n",
       " 'ёхталас',\n",
       " 'Наталья',\n",
       " 'нё̄тмил',\n",
       " 'о̄влэ̄т',\n",
       " '##колат',\n",
       " 'ӯщлах',\n",
       " '##ский',\n",
       " 'о̄ньщи',\n",
       " '##стит',\n",
       " 'ма̄нав',\n",
       " '##а̄ла',\n",
       " '##сныл',\n",
       " 'та̄гыл',\n",
       " 'нэ̄пак',\n",
       " 'е̄мтыс',\n",
       " 'ловинь',\n",
       " 'То̄рум',\n",
       " 'Никола',\n",
       " 'та̄рви',\n",
       " 'ва̄рыс',\n",
       " 'тэ̄лат',\n",
       " '##квет',\n",
       " 'щё̄пит',\n",
       " '##ккон',\n",
       " '##евич',\n",
       " 'хо̄тпа',\n",
       " 'Василь',\n",
       " 'э̄ргыт',\n",
       " 'школат',\n",
       " 'рӯпат',\n",
       " '##рись',\n",
       " 'Мансий',\n",
       " 'Россия',\n",
       " '##евна',\n",
       " 'та̄лыт',\n",
       " 'нё̄тнэ',\n",
       " '##круг',\n",
       " 'пӯмыщ',\n",
       " '##сӯв',\n",
       " '##таль',\n",
       " 'ва̄рыг',\n",
       " 'ла̄тыӈ',\n",
       " 'хансум',\n",
       " 'пормас',\n",
       " 'па̄выл',\n",
       " '##лэ̄т',\n",
       " '##олов',\n",
       " 'кӯщай',\n",
       " 'ще̄мья',\n",
       " 'янытыл',\n",
       " 'Наталь',\n",
       " 'ка̄сащ',\n",
       " '##лаве',\n",
       " 'патсыт',\n",
       " '##анум',\n",
       " 'э̄тпос',\n",
       " '##асум',\n",
       " '##авет',\n",
       " 'миркол',\n",
       " '##упыл',\n",
       " 'э̄лаль',\n",
       " 'ма̄хум',\n",
       " 'суссыл',\n",
       " 'районт',\n",
       " 'хансым',\n",
       " 'ла̄ве̄',\n",
       " '##о̄нт',\n",
       " 'ма̄ныл',\n",
       " '##̄пыӈ',\n",
       " '##овна',\n",
       " 'Ма̄хум',\n",
       " '##анув',\n",
       " 'рӯпит',\n",
       " 'аквъёт',\n",
       " '##хуйп',\n",
       " 'пусмал',\n",
       " '##осса',\n",
       " 'майвес',\n",
       " '##е̄кв',\n",
       " '##атем',\n",
       " '##сы̄г',\n",
       " '##ккар',\n",
       " 'ва̄рнэ',\n",
       " '##кола',\n",
       " 'кастыл',\n",
       " '##тэ̄н',\n",
       " '##ирыл',\n",
       " 'та̄нки',\n",
       " 'пуссын',\n",
       " '##льпи',\n",
       " 'о̄лсыт',\n",
       " 'Ка̄сыӈ',\n",
       " 'ма̄гыс',\n",
       " 'музейт',\n",
       " '##ысыт',\n",
       " 'хо̄тал',\n",
       " '##яныл',\n",
       " 'тэ̄нут',\n",
       " '##щтыр',\n",
       " 'тармыл',\n",
       " '##та̄л',\n",
       " '##таве',\n",
       " 'та̄нти',\n",
       " '##тэ̄в',\n",
       " '##алан',\n",
       " 'ка̄сыӈ',\n",
       " 'потрыт',\n",
       " '##раль',\n",
       " 'ла̄выс',\n",
       " 'ёмщакв',\n",
       " '##цият',\n",
       " 'патсум',\n",
       " '##ирма',\n",
       " '##аныл',\n",
       " '##лаль',\n",
       " '##аран',\n",
       " '##танэ',\n",
       " '##са̄т',\n",
       " '##ве̄н',\n",
       " '##асыт',\n",
       " '##уӈкв',\n",
       " '##матэ',\n",
       " '##о̄ль',\n",
       " '##па̄л',\n",
       " '##имир',\n",
       " '##щакв',\n",
       " 'пӯмща',\n",
       " 'а̄нумн',\n",
       " 'йильпи',\n",
       " 'хӯрум',\n",
       " '##ӈкве',\n",
       " 'таве̄н',\n",
       " '##ович',\n",
       " '##щлах',\n",
       " 'Сургут',\n",
       " 'хотьют',\n",
       " '##ме̄н',\n",
       " '##о̄ви',\n",
       " 'со̄тыр',\n",
       " 'маснут',\n",
       " '##санэ',\n",
       " '##маль',\n",
       " 'ка̄тыл',\n",
       " 'са̄вит',\n",
       " '##щхи',\n",
       " '##пал',\n",
       " 'матыр',\n",
       " '##лыл',\n",
       " 'ла̄тӈ',\n",
       " '##пар',\n",
       " '##кан',\n",
       " 'йӣкв',\n",
       " '##акв',\n",
       " '##стэ',\n",
       " '##а̄в',\n",
       " '##е̄г',\n",
       " '##ӈын',\n",
       " '##сия',\n",
       " 'па̄лт',\n",
       " 'ураль',\n",
       " '##сыр',\n",
       " '##хот',\n",
       " '##сыӈ',\n",
       " 'о̄лыс',\n",
       " '##ы̄н',\n",
       " 'патыс',\n",
       " 'тэ̄ла',\n",
       " '##лыӈ',\n",
       " '##там',\n",
       " '##лыг',\n",
       " '##мит',\n",
       " 'ха̄йт',\n",
       " 'то̄ва',\n",
       " '##янэ',\n",
       " '##тат',\n",
       " '##гум',\n",
       " 'хансы',\n",
       " '##анэ',\n",
       " 'хосыт',\n",
       " 'Алекс',\n",
       " '##нал',\n",
       " '##лан',\n",
       " '##кат',\n",
       " '##ерн',\n",
       " 'округ',\n",
       " '##вер',\n",
       " 'хольт',\n",
       " 'сунсы',\n",
       " 'Тамле',\n",
       " '##ова',\n",
       " '##лат',\n",
       " 'кӯща',\n",
       " '##ӈкв',\n",
       " '##сыт',\n",
       " '##тыл',\n",
       " '##гыт',\n",
       " '##пыӈ',\n",
       " '##ара',\n",
       " '##ган',\n",
       " 'нупыл',\n",
       " '##е̄т',\n",
       " '##ы̄г',\n",
       " '##э̄в',\n",
       " 'ве̄рм',\n",
       " '##рип',\n",
       " '##пат',\n",
       " 'ялпыӈ',\n",
       " '##сын',\n",
       " '##а̄л',\n",
       " '##льт',\n",
       " 'Ха̄ль',\n",
       " '##рий',\n",
       " '##ият',\n",
       " '##арт',\n",
       " '##пит',\n",
       " 'щирыл',\n",
       " '##кем',\n",
       " '##оль',\n",
       " '##аве',\n",
       " 'о̄луп',\n",
       " '##о̄х',\n",
       " '##о̄р',\n",
       " 'Ханты',\n",
       " '##ант',\n",
       " 'пыгыт',\n",
       " '##е̄р',\n",
       " '##мал',\n",
       " '##э̄н',\n",
       " '##рум',\n",
       " 'маныр',\n",
       " '##ови',\n",
       " 'культ',\n",
       " 'ханищ',\n",
       " '##щит',\n",
       " 'а̄гит',\n",
       " '##тум',\n",
       " '##тах',\n",
       " 'олныл',\n",
       " '##ный',\n",
       " '##о̄в',\n",
       " '##пыт',\n",
       " '##е̄м',\n",
       " 'Тувыл',\n",
       " 'тамле',\n",
       " '##ъёт',\n",
       " '##ниц',\n",
       " '##ныл',\n",
       " '##тыӈ',\n",
       " 'а̄нум',\n",
       " '##хо̄',\n",
       " 'о̄лнэ',\n",
       " '##мащ',\n",
       " 'молях',\n",
       " '##ван',\n",
       " '##вен',\n",
       " '##выл',\n",
       " '##тыг',\n",
       " '##ани',\n",
       " 'о̄ньщ',\n",
       " 'та̄лт',\n",
       " 'э̄руп',\n",
       " 'рӯпа',\n",
       " '##лам',\n",
       " 'То̄нт',\n",
       " '##мил',\n",
       " '##пак',\n",
       " 'музей',\n",
       " '##лым',\n",
       " '##вит',\n",
       " '##ник',\n",
       " '##мыг',\n",
       " 'патум',\n",
       " 'ма̄хм',\n",
       " '##ныг',\n",
       " 'хо̄нт',\n",
       " '##рыг',\n",
       " '##пыг',\n",
       " '##рим',\n",
       " '##сит',\n",
       " '##а̄н',\n",
       " '##пос',\n",
       " 'мирыт',\n",
       " 'ва̄ри',\n",
       " '##кве',\n",
       " 'Ма̄нь',\n",
       " '##гор',\n",
       " '##мам',\n",
       " '##кам',\n",
       " '##кыр',\n",
       " 'ханты',\n",
       " '##мат',\n",
       " 'хумус',\n",
       " 'колна',\n",
       " 'лю̄ль',\n",
       " 'сосса',\n",
       " '##иян',\n",
       " '##мус',\n",
       " '##е̄н',\n",
       " 'нампа',\n",
       " 'е̄мты',\n",
       " '##лас',\n",
       " 'та̄ра',\n",
       " 'ма̄нь',\n",
       " '##лыщ',\n",
       " '##алт',\n",
       " '##э̄г',\n",
       " '##лын',\n",
       " 'тотнэ',\n",
       " '##онт',\n",
       " '##тым',\n",
       " 'хульт',\n",
       " '##мле',\n",
       " '##ина',\n",
       " '##лыт',\n",
       " '##сыг',\n",
       " 'та̄лэ',\n",
       " '##о̄п',\n",
       " '##тал',\n",
       " 'хурит',\n",
       " '##тыт',\n",
       " 'хӯнт',\n",
       " '##хат',\n",
       " '##ман',\n",
       " 'ляпат',\n",
       " 'мощща',\n",
       " '##гыл',\n",
       " 'янмал',\n",
       " '##та̄',\n",
       " 'ро̄ви',\n",
       " '##е̄в',\n",
       " 'лё̄ӈх',\n",
       " '##лов',\n",
       " '##пан',\n",
       " 'ка̄са',\n",
       " 'па̄вл',\n",
       " 'во̄ра',\n",
       " '##ана',\n",
       " 'ӯрга',\n",
       " 'щё̄пи',\n",
       " '##лад',\n",
       " '##сум',\n",
       " 'школа',\n",
       " 'во̄рт',\n",
       " '##сам',\n",
       " '##нав',\n",
       " '##сыл',\n",
       " '##рис',\n",
       " 'акваг',\n",
       " '##ква',\n",
       " 'Татья',\n",
       " '##рыт',\n",
       " '##гра',\n",
       " '##рам',\n",
       " 'ва̄ти',\n",
       " '##пыл',\n",
       " '##а̄т',\n",
       " '##тыс',\n",
       " 'Округ',\n",
       " 'потыр',\n",
       " '##лыс',\n",
       " '##ӯв',\n",
       " 'манос',\n",
       " 'ща̄нь',\n",
       " 'мо̄йт',\n",
       " '##ве̄',\n",
       " '##нув',\n",
       " 'район',\n",
       " '##рия',\n",
       " 'тувыл',\n",
       " '##шин',\n",
       " 'такви',\n",
       " 'ва̄нт',\n",
       " '##тын',\n",
       " '##мас',\n",
       " '##атэ',\n",
       " 'самын',\n",
       " '##хум',\n",
       " 'колыт',\n",
       " '##тас',\n",
       " 'хорам',\n",
       " 'э̄рнэ',\n",
       " '##ила',\n",
       " '##ест',\n",
       " '##о̄м',\n",
       " '##луп',\n",
       " '##сий',\n",
       " 'акван',\n",
       " '##ент',\n",
       " '##̄кв',\n",
       " '##сал',\n",
       " '##лум',\n",
       " 'о̄щнэ',\n",
       " '##вет',\n",
       " '##ков',\n",
       " 'порат',\n",
       " '##э̄т',\n",
       " '##кол',\n",
       " 'о̄лум',\n",
       " '##иль',\n",
       " '##кру',\n",
       " 'э̄ква',\n",
       " 'то̄нт',\n",
       " '##ель',\n",
       " '##тыр',\n",
       " '##ург',\n",
       " '##ура',\n",
       " '##лты',\n",
       " '##нэ̄',\n",
       " '##а̄г',\n",
       " '##пас',\n",
       " 'рӯпи',\n",
       " 'о̄йка',\n",
       " '##тем',\n",
       " '##ция',\n",
       " '##тан',\n",
       " 'Саран',\n",
       " '##итэ',\n",
       " '##лах',\n",
       " 'ёхтыс',\n",
       " 'ёхтал',\n",
       " '##ева',\n",
       " '##вес',\n",
       " '##рищ',\n",
       " '##нут',\n",
       " '##хал',\n",
       " '##тап',\n",
       " 'са̄лы',\n",
       " '##тнэ',\n",
       " 'минас',\n",
       " '##ьют',\n",
       " 'о̄выл',\n",
       " 'а̄тим',\n",
       " '##нти',\n",
       " 'ла̄ви',\n",
       " '##увт',\n",
       " 'касыл',\n",
       " '##ан',\n",
       " '##рг',\n",
       " '##пи',\n",
       " '##др',\n",
       " 'пе̄с',\n",
       " '##ир',\n",
       " '##ӯ',\n",
       " '##̄м',\n",
       " '##ой',\n",
       " '##ел',\n",
       " '##ий',\n",
       " '##на',\n",
       " '##ен',\n",
       " '##ус',\n",
       " '##га',\n",
       " '##ис',\n",
       " '##уй',\n",
       " '##рм',\n",
       " 'наме',\n",
       " '##ав',\n",
       " 'са̄в',\n",
       " '##уп',\n",
       " '##он',\n",
       " 'газе',\n",
       " '##яр',\n",
       " 'ва̄р',\n",
       " '##за',\n",
       " '##ла',\n",
       " '##ег',\n",
       " '##ол',\n",
       " 'музе',\n",
       " '##ащ',\n",
       " '##э̄',\n",
       " 'нама',\n",
       " '##аг',\n",
       " '##ъя',\n",
       " '##но',\n",
       " '##ты',\n",
       " '##оз',\n",
       " '##ас',\n",
       " '##эн',\n",
       " '##ум',\n",
       " 'ка̄т',\n",
       " '##си',\n",
       " '##ве',\n",
       " 'Мо̄т',\n",
       " '##ли',\n",
       " '##та',\n",
       " 'а̄ти',\n",
       " '##те',\n",
       " 'по̄х',\n",
       " '##ыщ',\n",
       " 'Серг',\n",
       " '##ек',\n",
       " '##иг',\n",
       " 'такв',\n",
       " 'ма̄т',\n",
       " '##де',\n",
       " '##ын',\n",
       " '##ни',\n",
       " 'сапр',\n",
       " 'ёхты',\n",
       " '##̄л',\n",
       " '##ин',\n",
       " 'колт',\n",
       " '##хв',\n",
       " 'номс',\n",
       " '##ар',\n",
       " 'ёмас',\n",
       " 'ӯст',\n",
       " 'а̄ги',\n",
       " 'ӯнт',\n",
       " 'хо̄н',\n",
       " 'урыл',\n",
       " '##ви',\n",
       " '##юм',\n",
       " '##ут',\n",
       " '##ущ',\n",
       " '##хи',\n",
       " '##тр',\n",
       " 'Свет',\n",
       " '##зе',\n",
       " 'колн',\n",
       " '##па',\n",
       " 'ва̄т',\n",
       " 'потр',\n",
       " '##ну',\n",
       " '##ыӈ',\n",
       " '##щи',\n",
       " 'йӣв',\n",
       " '##ья',\n",
       " 'е̄мт',\n",
       " '##ху',\n",
       " 'па̄в',\n",
       " 'ма̄н',\n",
       " '##об',\n",
       " 'та̄л',\n",
       " 'ла̄х',\n",
       " 'на̄й',\n",
       " '##са',\n",
       " '##ес',\n",
       " 'арыг',\n",
       " '##е̄',\n",
       " '##ом',\n",
       " 'но̄х',\n",
       " 'пӯм',\n",
       " '##ми',\n",
       " 'та̄р',\n",
       " 'Тэ̄н',\n",
       " '##лн',\n",
       " 'ю̄нт',\n",
       " '##да',\n",
       " '##ур',\n",
       " 'ё̄рн',\n",
       " '##ью',\n",
       " 'нэ̄м',\n",
       " '##ия',\n",
       " '##ги',\n",
       " 'лю̄л',\n",
       " '##от',\n",
       " '##ый',\n",
       " '##о̄',\n",
       " 'ювле',\n",
       " '##лт',\n",
       " 'хоса',\n",
       " 'пуӈк',\n",
       " '##сы',\n",
       " '##ге',\n",
       " '##ва',\n",
       " '##ер',\n",
       " '##ыл',\n",
       " 'Пе̄с',\n",
       " '##кв',\n",
       " 'пора',\n",
       " 'о̄нь',\n",
       " '##ки',\n",
       " '##ым',\n",
       " 'ма̄г',\n",
       " 'тэ̄н',\n",
       " '##ор',\n",
       " 'мо̄т',\n",
       " 'сака',\n",
       " 'са̄л',\n",
       " 'хуль',\n",
       " 'паты',\n",
       " 'нё̄т',\n",
       " 'тарм',\n",
       " '##ув',\n",
       " 'нэ̄г',\n",
       " 'во̄р',\n",
       " '##ян',\n",
       " 'хӯл',\n",
       " 'хунь',\n",
       " '##кк',\n",
       " 'ӯсн',\n",
       " '##нь',\n",
       " 'ялас',\n",
       " '##кт',\n",
       " '##ща',\n",
       " '##ищ',\n",
       " 'па̄л',\n",
       " '##ль',\n",
       " 'сунс',\n",
       " '##ай',\n",
       " '##вт',\n",
       " '##ил',\n",
       " '##ыр',\n",
       " 'ма̄х',\n",
       " '##ох',\n",
       " '##ци',\n",
       " 'Куль',\n",
       " 'халт',\n",
       " 'аквт',\n",
       " '##яс',\n",
       " 'мӯй',\n",
       " '##оп',\n",
       " '##ыт',\n",
       " '##пр',\n",
       " '##лу',\n",
       " 'таӈх',\n",
       " '##ыс',\n",
       " '##ед',\n",
       " 'таве',\n",
       " '##̄т',\n",
       " '##ет',\n",
       " '##ны',\n",
       " '##тэ',\n",
       " '##рт',\n",
       " 'то̄в',\n",
       " '##ъё',\n",
       " '##ик',\n",
       " 'Ма̄н',\n",
       " 'Югра',\n",
       " '##ам',\n",
       " '##тӈ',\n",
       " 'ялан',\n",
       " '##пу',\n",
       " 'са̄т',\n",
       " 'мӯс',\n",
       " '##ка',\n",
       " '##00',\n",
       " 'Иван',\n",
       " 'ёхта',\n",
       " 'мирн',\n",
       " 'Яныг',\n",
       " '##ап',\n",
       " 'нила',\n",
       " 'на̄н',\n",
       " '##̄р',\n",
       " '##ха',\n",
       " 'ӯрг',\n",
       " '##ко',\n",
       " 'Сург',\n",
       " '##кс',\n",
       " '##ог',\n",
       " '##ит',\n",
       " '##ре',\n",
       " '##ос',\n",
       " 'а̄ст',\n",
       " '##нэ',\n",
       " '##а̄',\n",
       " '##̄с',\n",
       " '##ун',\n",
       " 'клас',\n",
       " 'То̄в',\n",
       " '##ие',\n",
       " 'ханс',\n",
       " 'во̄в',\n",
       " 'Влад',\n",
       " '##ич',\n",
       " '##ат',\n",
       " '##оч',\n",
       " '##ск',\n",
       " 'Та̄н',\n",
       " '##лы',\n",
       " '##ем',\n",
       " 'лӯп',\n",
       " 'ха̄й',\n",
       " '##ти',\n",
       " '##ок',\n",
       " 'ща̄г',\n",
       " 'ке̄т',\n",
       " '##̄н',\n",
       " '##нг',\n",
       " '##ле',\n",
       " '##ст',\n",
       " 'стих',\n",
       " '##ри',\n",
       " 'яныг',\n",
       " '##йп',\n",
       " '##од',\n",
       " '##йт',\n",
       " 'Хунь',\n",
       " '##ыг',\n",
       " 'ла̄в',\n",
       " 'рӯп',\n",
       " '##ах',\n",
       " 'алыщ',\n",
       " '##ях',\n",
       " '##̄в',\n",
       " '##нт',\n",
       " '##ев',\n",
       " '##̄й',\n",
       " 'э̄кв',\n",
       " 'а̄гм',\n",
       " 'коны',\n",
       " 'ще̄м',\n",
       " '##ра',\n",
       " '##ме',\n",
       " 'Во̄р',\n",
       " 'хури',\n",
       " '##̄г',\n",
       " '##ал',\n",
       " '##во',\n",
       " '##им',\n",
       " '##ӈх',\n",
       " 'ма̄к',\n",
       " 'хо̄т',\n",
       " 'солк',\n",
       " 'по̄с',\n",
       " 'Са̄в',\n",
       " '##ру',\n",
       " '##ё̄',\n",
       " 'ва̄г',\n",
       " '##ӈк',\n",
       " 'яныт',\n",
       " '##иӈ',\n",
       " 'о̄лы',\n",
       " 'палт',\n",
       " 'э̄рг',\n",
       " 'а̄тя',\n",
       " '##дм',\n",
       " '##ту',\n",
       " 'э̄ри',\n",
       " '##ов',\n",
       " '##ма',\n",
       " '##ям',\n",
       " 'та̄н',\n",
       " 'э̄лы',\n",
       " '##ей',\n",
       " 'туи',\n",
       " 'Ком',\n",
       " 'пе̄',\n",
       " '##Щ',\n",
       " '##ö',\n",
       " 'хос',\n",
       " 'ном',\n",
       " '##j',\n",
       " '##r',\n",
       " 'Хӯ',\n",
       " '##и',\n",
       " 'ла̄',\n",
       " '##R',\n",
       " '##з',\n",
       " 'кол',\n",
       " '##S',\n",
       " 'Пе̄',\n",
       " '##о',\n",
       " 'суп',\n",
       " '##щ',\n",
       " 'е̄м',\n",
       " '##m',\n",
       " 'во̄',\n",
       " '##4',\n",
       " '##Ю',\n",
       " '##E',\n",
       " 'тох',\n",
       " '##ѐ',\n",
       " '##М',\n",
       " '##¸',\n",
       " 'ойт',\n",
       " 'Вал',\n",
       " 'ке̄',\n",
       " '##F',\n",
       " 'сов',\n",
       " 'сам',\n",
       " '##Н',\n",
       " 'сыс',\n",
       " 'пас',\n",
       " 'рӯ',\n",
       " 'ёма',\n",
       " 'э̄р',\n",
       " '##ф',\n",
       " 'тав',\n",
       " 'Тэ̄',\n",
       " 'Тох',\n",
       " '##ю',\n",
       " 'па̄',\n",
       " '##п',\n",
       " 'туп',\n",
       " 'Хо̄',\n",
       " '##ч',\n",
       " 'хот',\n",
       " '##ў',\n",
       " 'мас',\n",
       " '##×',\n",
       " 'сыр',\n",
       " 'сэ̄',\n",
       " '##ж',\n",
       " '##p',\n",
       " 'хум',\n",
       " 'вит',\n",
       " '##С',\n",
       " '##О',\n",
       " '##В',\n",
       " 'рущ',\n",
       " '##ü',\n",
       " 'кит',\n",
       " 'мол',\n",
       " 'кӯ',\n",
       " '##Z',\n",
       " 'нам',\n",
       " '##k',\n",
       " '##ó',\n",
       " '##i',\n",
       " 'сӯ',\n",
       " 'ӯн',\n",
       " 'кин',\n",
       " 'ман',\n",
       " '##Ш',\n",
       " '##ӈ',\n",
       " 'Лӯ',\n",
       " 'мин',\n",
       " 'мощ',\n",
       " 'сол',\n",
       " '##Р',\n",
       " 'сти',\n",
       " 'сун',\n",
       " '201',\n",
       " 'лю̄',\n",
       " '##ӑ',\n",
       " '##Y',\n",
       " 'вос',\n",
       " 'ё̄м',\n",
       " '##Э',\n",
       " '##Д',\n",
       " '##ӊ',\n",
       " ...]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(new_wordpiece.get_vocab(), key=lambda x: len(x), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c29221a0-22f3-484e-b82e-2fc56e548a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 767 new tokens to tokenizer\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer, new_tokens = add_tokens(list(new_wordpiece.get_vocab().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3d072233-1cb2-4ab3-afc2-7dc767a58bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TKN_DIR_OLD = 'labse_tokenizer'\n",
    "TKN_DIR_NEW = 'new_tokenizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "049f517c-ef08-4f0c-a517-2bdbb4264090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_conflicting_tokens(new_tokens, old_vocab):\n",
    "    conflicting_tokens = []\n",
    "    \n",
    "    for token in tqdm(new_tokens):\n",
    "        if token.startswith(\"##\"):\n",
    "            # Убедимся, что корень не существует в старом словаре\n",
    "            root_token = token[2:]  # Убираем '##'\n",
    "            if root_token in old_vocab:\n",
    "                conflicting_tokens.append(root_token)\n",
    "        else:\n",
    "            # Проверяем, что не существует подслов с этим корнем\n",
    "            subword_token = f\"##{token}\"\n",
    "            if subword_token in old_vocab:\n",
    "                conflicting_tokens.append(subword_token)\n",
    "    \n",
    "    return conflicting_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "662517e1-fb0b-4bf1-a476-09b669afef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_conflicting_tokens(conflicting_tokens, old_vocab):\n",
    "    cleaned_vocab = deepcopy(old_vocab)\n",
    "    \n",
    "    for token in conflicting_tokens:\n",
    "        if token in cleaned_vocab:\n",
    "            del cleaned_vocab[token]\n",
    "    \n",
    "    return cleaned_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "18ee880e-4f5c-403a-829d-8dc902a286d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def update_tokenizer_with_clean_vocab(new_tokens):\n",
    "    old_tokenizer = deepcopy(labse.tokenizer)\n",
    "    \n",
    "    #conflicting_tokens = find_conflicting_tokens(new_tokens, old_tokenizer.vocab)\n",
    "    #cleaned_vocab = remove_conflicting_tokens(conflicting_tokens, old_tokenizer.vocab)\n",
    "\n",
    "    with open(f\"{TKN_DIR_OLD}/tokenizer.json\", \"r\", encoding='utf-8') as f:\n",
    "        cfg = json.load(f)\n",
    "    \n",
    "    # Обновляем словарь в токенизаторе\n",
    "    cfg['model']['vocab'] = cleaned_vocab\n",
    "\n",
    "    with open(f\"{TKN_DIR_NEW}/tokenizer.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(cfg, f, indent=False)\n",
    "\n",
    "    with open(f\"{TKN_DIR_NEW}/vocab.txt\", \"w\", encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(list(cfg['model']['vocab'].keys())))\n",
    "\n",
    "    new_tokenizer = AutoTokenizer.from_pretrained(TKN_DIR_NEW)\n",
    "    assert len(new_tokenizer.vocab) != 0\n",
    "    # Добавляем новые токены\n",
    "    new_tokens_to_add = list(set(new_tokens) - set(new_tokenizer.vocab.keys()))\n",
    "    new_tokenizer.add_tokens(new_tokens_to_add)\n",
    "    \n",
    "    return new_tokenizer, new_tokens_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "32dc6778-7267-4310-bda0-c7df29eb1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tokens_resave(vocab):\n",
    "    new_tokenizer = deepcopy(labse.tokenizer)\n",
    "    new_tokens = list(set(vocab) - set(new_tokenizer.vocab.keys()))\n",
    "    num_tokens_added = new_tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "    with open(f\"{TKN_DIR_OLD}/tokenizer.json\", \"r\", encoding='utf-8') as f:\n",
    "        cfg = json.load(f)\n",
    "    \n",
    "    cfg['model']['vocab'] = new_tokenizer.vocab\n",
    "    \n",
    "    with open(f\"{TKN_DIR_NEW}/tokenizer.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(cfg, f, indent=False)\n",
    "\n",
    "    with open(f\"{TKN_DIR_NEW}/vocab.txt\", \"w\", encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(list(cfg['model']['vocab'].keys())))\n",
    "    new_tokenizer = AutoTokenizer.from_pretrained(TKN_DIR_NEW, use_fast=False)\n",
    "    print(f'Added {num_tokens_added} new tokens to tokenizer')\n",
    "    return new_tokenizer, new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "e9414bff-518a-4e20-b494-35b851c68392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 767 new tokens to tokenizer\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer, new_tokens = add_tokens_resave(list(new_wordpiece.get_vocab().keys())) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fb45ca79-dfb2-4400-8aee-6f8f856781c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenizers(idx, old_tokenizer=old_tokenizer, new_tokenizer=new_tokenizer):\n",
    "    source = data.source.loc[idx]\n",
    "    target = data.target.loc[idx]\n",
    "    print('Orig mansi:')\n",
    "    print(target)\n",
    "    #print()\n",
    "    print('Old tokenizer on mansi:')\n",
    "    l1 = old_tokenizer.convert_ids_to_tokens(old_tokenizer.encode(target))\n",
    "    print(l1)\n",
    "    #print()\n",
    "    l11 = old_tokenizer.decode(old_tokenizer.encode(target)[1:-1])\n",
    "    print('New tokenizer on mansi:')\n",
    "    l2 = new_tokenizer.convert_ids_to_tokens(new_tokenizer.encode(target))\n",
    "    print(l2)\n",
    "    #print()\n",
    "    l22 = new_tokenizer.decode(new_tokenizer.encode(target)[1:-1])\n",
    "    print(\"Tokenized equally:\", l1 == l2)\n",
    "    is_equal_decode = l11 == l22 == target\n",
    "    if not is_equal_decode:\n",
    "        print('Decoded not equally')\n",
    "        print('Decoded on old:', l11)\n",
    "        print('Decoded on new:', l22)\n",
    "    else:\n",
    "        print('Decoded equally')\n",
    "    print()\n",
    "    print('Orig rus:')\n",
    "    print(source)\n",
    "    print('Old tokenizer on rus:')\n",
    "    l1 = old_tokenizer.convert_ids_to_tokens(old_tokenizer.encode(source))\n",
    "    print(l1)\n",
    "    l11 = old_tokenizer.decode(old_tokenizer.encode(source)[1:-1])\n",
    "    #print()\n",
    "    print('New tokenizer on rus:')\n",
    "    l2 = new_tokenizer.convert_ids_to_tokens(new_tokenizer.encode(source))\n",
    "    print(l2)\n",
    "    l22 = new_tokenizer.decode(new_tokenizer.encode(source)[1:-1])\n",
    "    #print()\n",
    "    print(\"Tokenized equally:\", l1 == l2)\n",
    "    is_equal_decode = l11 == l22 == source\n",
    "    if not is_equal_decode:\n",
    "        print('Decoded not equally')\n",
    "        print('Decoded on old:', l11)\n",
    "        print('Decoded on new:', l22)\n",
    "    else:\n",
    "        print('Decoded equally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3bb00ccb-b285-450a-9c54-2facd73ec301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([   61,    65,   135,   152,   178,   386,   393,   481,   491,  1136,\n",
       "       ...\n",
       "       75672, 75734, 76334, 76689, 76908, 76938, 77139, 77295, 77316, 77334],\n",
       "      dtype='int64', length=2286)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.target.str.contains(\"ханищта\")].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "bbf65ee8-6acc-480f-940e-e6def05de17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Так', '##ви', 'па', '##лт', '##э', 'о', '##̄', '##нь', '##ща', '##ст', '##э', ',', 'ян', '##мал', '##тас', '##тэ', ',', '[UNK]', 'ос', 'та', '##̄', '##кса', '##рыг', '[UNK]', 'хан', '##ищ', '##тас', '##тэ', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(data.target.loc[152], old_tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "f0422902-e76b-4f0b-84d3-9c11dfd825f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Так', '##ви', 'палт', '##э', 'о̄ньщ', '##аст', '##э', ',', 'янмал', '##тастэ', ',', 'рӯпит', '##а', '##ӈкв', 'ос', 'та̄', '##кса', '##рыг', 'о̄л', '##уӈкв', 'ханищ', '##тастэ', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(data.target.loc[152], new_tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "d787dca1-642c-4d4e-b368-9372c7b3f3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Такви палтэ о̄ньщастэ, янмалтастэ, рӯпитаӈкв ос та̄ксарыг о̄луӈкв ханищтастэ. [SEP]'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.decode(token2ids(tokenize(data.target.loc[152], new_tokenizer.vocab), new_tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "a8ae6c9c-36d3-4c7c-8141-5ef2ba5ad652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1645286\\venvs\\py310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer2 = AutoTokenizer.from_pretrained(TKN_DIR_NEW, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "e8404b5e-1360-4898-b388-6be29cb573b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11d511e19c644e693a3424c34c46b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nas = []\n",
    "for i in tqdm(range(2000)):\n",
    "    if not new_tokenizer.encode(data.target.loc[i]) == token2ids(tokenize(data.target.loc[i], new_tokenizer.vocab), new_tokenizer.vocab):\n",
    "        nas.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "5357723a-1f08-47fc-a1fd-75fd22104640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 6, 8, 13]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "db59ab8b-e606-4356-8fcc-8ff697000f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ман ты пӣлтал, веськат хумиюв нэтхуньт ат ёрувлвылув'"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 4\n",
    "data.target.loc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "a6fe09d8-434d-478b-adf6-c93a423f0fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'̄л'"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'пӣлтал'[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "a32c7477-332a-4eb6-9573-999451ebedcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'̄л' in new_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "abc40c79-9360-4d59-b46f-a08a66d9a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Ман', 'ты', 'пи', '##̄л', '##тал', ',', 'весь', '##кат', 'хум', '##ию', '##в', 'н', '##эт', '##ху', '##нь', '##т', 'ат', 'ё', '##ру', '##в', '##лв', '##ылу', '##в', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(data.target.loc[i], new_tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "af769b3f-a7f8-4ca6-82e0-ac706a201ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Ман', 'ты', 'п', '##ӣ', '##лта', '##л', ',', 'весь', '##кат', 'хум', '##ию', '##в', 'н', '##эт', '##ху', '##нь', '##т', 'ат', 'ё', '##ру', '##в', '##лв', '##ылу', '##в', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer.convert_ids_to_tokens(new_tokenizer.encode(data.target.loc[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "839aff61-e0f2-42b7-8eea-98498214be0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 387098, 82546, 382840, 1007, 212162, 270774, 491731, 172342, 341765, 400044, 116046, 439312, 341134, 306762, 500300, 204659, 348463, 201819, 7925, 110251, 439312, 173324, 41862, 439312, 102]\n",
      "[101, 387098, 82546, 132601, 409473, 376164, 491731, 172342, 341765, 400044, 116046, 439312, 341134, 306762, 500300, 204659, 348463, 201819, 7925, 110251, 439312, 173324, 41862, 439312, 102]\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer.encode(data.target.loc[i]))\n",
    "print(token2ids(tokenize(data.target.loc[i], new_tokenizer.vocab), new_tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "93fbc60a-be40-4c63-930c-fe89fb16df30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'##ӯр' in new_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "a6d7e33d-d95d-4eaf-aedb-af7e47ce149c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig mansi:\n",
      "Такви палтэ о̄ньщастэ, янмалтастэ, рӯпитаӈкв ос та̄ксарыг о̄луӈкв ханищтастэ.\n",
      "Old tokenizer on mansi:\n",
      "['[CLS]', 'Так', '##ви', 'па', '##лт', '##э', 'о', '##̄', '##нь', '##ща', '##ст', '##э', ',', 'ян', '##мал', '##тас', '##тэ', ',', '[UNK]', 'ос', 'та', '##̄', '##кса', '##рыг', '[UNK]', 'хан', '##ищ', '##тас', '##тэ', '.', '[SEP]']\n",
      "New tokenizer on mansi:\n",
      "['[CLS]', 'Так', '##ви', 'палт', '##э', 'о̄ньщ', '##аст', '##э', ',', 'янмал', '##тастэ', ',', 'рӯ', '##пита', '##ӈкв', 'ос', 'та̄', '##кса', '##рыг', 'о̄л', '##уӈкв', 'ханищ', '##тастэ', '.', '[SEP]']\n",
      "Tokenized equally: False\n",
      "Decoded not equally\n",
      "Decoded on old: Такви палтэ о̄ньщастэ, янмалтастэ, [UNK] ос та̄ксарыг [UNK] ханищтастэ.\n",
      "Decoded on new: Такви палтэ о̄ньщастэ, янмалтастэ, рӯпитаӈкв ос та̄ксарыг о̄луӈкв ханищтастэ.\n",
      "\n",
      "Orig rus:\n",
      "Он вырастил и воспитал мальчика, привив ему трудолюбие и твёрдый характер.\n",
      "Old tokenizer on rus:\n",
      "['[CLS]', 'Он', 'вы', '##расти', '##л', 'и', 'во', '##сп', '##ита', '##л', 'мальчика', ',', 'прив', '##ив', 'ему', 'труд', '##ол', '##юб', '##ие', 'и', 'тв', '##ёр', '##дый', 'характер', '.', '[SEP]']\n",
      "New tokenizer on rus:\n",
      "['[CLS]', 'Он', 'вы', '##расти', '##л', 'и', 'вос', '##пита', '##л', 'мальчика', ',', 'прив', '##ив', 'ему', 'труд', '##ол', '##юб', '##ие', 'и', 'тв', '##ёр', '##дый', 'характер', '.', '[SEP]']\n",
      "Tokenized equally: False\n",
      "Decoded equally\n"
     ]
    }
   ],
   "source": [
    "test_tokenizers(152, old_tokenizer, new_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad8f04-9deb-4d2d-a112-6e0327d0f275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a3f98650-a4af-4ef1-b571-5f2ed786558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, vocab):\n",
    "    tokens = []\n",
    "    c = 0\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        start_idx = 2 if word.startswith('##') else 0\n",
    "        while i-start_idx > 0 and word[:i] not in vocab:\n",
    "            #print(word[:i])\n",
    "            i -= 1\n",
    "        if i-start_idx == 0:\n",
    "            return ['[UNK]']\n",
    "        #print(i, word[:i])\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "        #print(word)\n",
    "        c += 1\n",
    "        if c > 10:\n",
    "            break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "dc4645f0-46b1-487e-92ef-ec32d501ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts, vocab):\n",
    "    is_str = False\n",
    "    if isinstance(texts, str):\n",
    "        is_str = True\n",
    "        texts = [texts]\n",
    "    tokens = []\n",
    "    for text in texts:\n",
    "        pre_tokenize_result = old_tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "        pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "        tokens.append([encode_word(word, vocab) for word in pre_tokenized_text])\n",
    "    tokens = [['[CLS]'] + sum(ts, []) + ['[SEP]'] for ts in tokens]\n",
    "    return tokens if not is_str else tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "1ed9cc0d-0493-49b4-8fd1-fc5fac86d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2ids(tokens, vocab):\n",
    "    special = {\n",
    "        '[PAD]': 0,\n",
    "        '[UNK]': 100,\n",
    "        \"[CLS]\": 101,\n",
    "        \"[SEP]\": 102,\n",
    "        \"[MASK]\": 103\n",
    "    }\n",
    "    return [special[token] if token in special else vocab[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "92db8bd4-7a39-4ce0-918d-da65d9278fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Та',\n",
       " '##кв',\n",
       " '##и',\n",
       " 'палт',\n",
       " '##э',\n",
       " 'о̄ньщ',\n",
       " '##ас',\n",
       " '##тэ',\n",
       " ',',\n",
       " 'янмал',\n",
       " '##тастэ',\n",
       " ',',\n",
       " 'рӯпит',\n",
       " '##а',\n",
       " '##ӈкв',\n",
       " 'ос',\n",
       " 'та̄',\n",
       " '##кс',\n",
       " '##ар',\n",
       " '##ыг',\n",
       " 'о̄л',\n",
       " '##уӈкв',\n",
       " 'ханищ',\n",
       " '##тастэ',\n",
       " '.']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[new_wordpiece.id_to_token(idx) for idx in new_wordpiece.encode(data.target.loc[152]).ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0257bfd-2a8f-4f82-81c9-a32a11430ad4",
   "metadata": {},
   "source": [
    "# train new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61409e77-86fa-4c8b-bba0-9425ab812dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import models, pre_tokenizers, decoders, Tokenizer\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad10a40f-7147-4db6-a698-b04fee86f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wordpiece = Tokenizer(models.WordPiece())\n",
    "new_wordpiece.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=1200,\n",
    "    min_frequency=30,\n",
    "    show_progress=True,\n",
    "    continuing_subword_prefix='##'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6288c4f-b0c3-4330-a0cf-68a37a71475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wordpiece.train_from_iterator(all_corpus, trainer=trainer, length=len(all_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5509c2b9-a328-46f9-a298-8f54d724665d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ханищтах', 'ханищтан']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[el for el in sorted(new_wordpiece.get_vocab(), key=lambda x: len(x), reverse=True) if 'ханищта' in el]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e18a115-e5f8-4bd9-8351-b28b430cfc88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['э̄лумхо̄лас',\n",
       " 'рӯпитаӈкве',\n",
       " 'Саранпа̄выл',\n",
       " 'рӯпитэ̄гыт',\n",
       " '##лумхо̄лас',\n",
       " 'потыртасыт',\n",
       " '##таве̄сыт',\n",
       " '##щхӣпыӈ',\n",
       " 'хо̄талэ̄т',\n",
       " 'ва̄руӈкве',\n",
       " '##има̄гыс',\n",
       " 'на̄врамыт',\n",
       " 'ёхталасыт',\n",
       " 'Александр',\n",
       " 'ва̄рмалит',\n",
       " 'ня̄врамыт',\n",
       " 'ма̄хманув',\n",
       " '##е̄ккар',\n",
       " '##а̄врам',\n",
       " '##лэ̄гыт',\n",
       " '##луӈкве',\n",
       " 'ва̄рмаль',\n",
       " 'потыртас',\n",
       " '##хо̄лас',\n",
       " '##па̄выл',\n",
       " 'рӯпитан',\n",
       " 'о̄луӈкве',\n",
       " 'нэ̄пакыт',\n",
       " 'ма̄ньлат',\n",
       " 'пӯльниц',\n",
       " '##туӈкве',\n",
       " 'Мансийск',\n",
       " 'ханищтах',\n",
       " '##тыяныл',\n",
       " '##тэ̄гыт',\n",
       " 'о̄вылтах',\n",
       " 'ищхӣпыӈ',\n",
       " 'та̄наныл',\n",
       " '##таӈкве',\n",
       " 'Ха̄льӯс',\n",
       " 'рӯпитас',\n",
       " '##ве̄сыт',\n",
       " 'о̄лэ̄гыт',\n",
       " 'ханищтан',\n",
       " 'халанылт',\n",
       " 'округувт',\n",
       " 'пормасыт',\n",
       " '##лаӈкве',\n",
       " '##тыглас',\n",
       " '##рищит',\n",
       " '##э̄гыт',\n",
       " '##анылт',\n",
       " '##лавес',\n",
       " 'ва̄рмал',\n",
       " '##о̄рум',\n",
       " '##ияныл',\n",
       " '##тасум',\n",
       " 'ма̄ньщи',\n",
       " 'о̄влэ̄т',\n",
       " 'ле̄ккар',\n",
       " 'на̄врам',\n",
       " '##уӈкве',\n",
       " 'ня̄врам',\n",
       " '##а̄гыл',\n",
       " '##е̄гыт',\n",
       " 'Алексан',\n",
       " 'са̄всыр',\n",
       " 'рӯпата',\n",
       " '##тавес',\n",
       " 'па̄вылт',\n",
       " '##̄врам',\n",
       " 'мӯйлуп',\n",
       " '##таӈкв',\n",
       " 'рӯпиты',\n",
       " '##ласыт',\n",
       " '##тавет',\n",
       " 'ёмащакв',\n",
       " '##овинь',\n",
       " '##а̄выл',\n",
       " 'яныгмас',\n",
       " '##саныл',\n",
       " '##е̄гум',\n",
       " 'со̄тыра',\n",
       " 'кӯщаит',\n",
       " '##колат',\n",
       " 'па̄вылн',\n",
       " 'хо̄тпат',\n",
       " 'ма̄щтыр',\n",
       " '##има̄г',\n",
       " '##тыгла',\n",
       " '##хатыг',\n",
       " 'Наталья',\n",
       " 'ма̄гсыл',\n",
       " 'ла̄тӈыл',\n",
       " 'хо̄тпаг',\n",
       " '##тасыт',\n",
       " 'э̄тпост',\n",
       " '##янӯв',\n",
       " 'Ха̄льӯ',\n",
       " '##э̄гум',\n",
       " '##льниц',\n",
       " '##таве',\n",
       " 'школат',\n",
       " 'Мансий',\n",
       " '##сы̄г',\n",
       " '##тэ̄в',\n",
       " 'ма̄ныл',\n",
       " 'суссыл',\n",
       " '##круг',\n",
       " '##яныл',\n",
       " 'пормас',\n",
       " 'ловинь',\n",
       " '##имир',\n",
       " '##лэ̄т',\n",
       " '##лаль',\n",
       " '##ирыл',\n",
       " 'па̄выл',\n",
       " 'То̄рум',\n",
       " '##о̄нт',\n",
       " 'кастыл',\n",
       " 'ка̄сыӈ',\n",
       " 'Россия',\n",
       " 'та̄нки',\n",
       " '##та̄л',\n",
       " '##ме̄н',\n",
       " 'йильпи',\n",
       " 'рӯпит',\n",
       " '##аран',\n",
       " '##а̄ла',\n",
       " 'э̄лаль',\n",
       " 'ла̄выс',\n",
       " 'о̄ньщи',\n",
       " 'Никола',\n",
       " 'ма̄хум',\n",
       " 'хо̄тал',\n",
       " '##кола',\n",
       " '##ккар',\n",
       " '##ский',\n",
       " 'потрыт',\n",
       " '##̄пыӈ',\n",
       " '##лаве',\n",
       " 'о̄лсыт',\n",
       " 'нё̄тнэ',\n",
       " 'янытыл',\n",
       " 'кӯщай',\n",
       " '##олов',\n",
       " 'ла̄ве̄',\n",
       " 'ва̄рнэ',\n",
       " '##раль',\n",
       " 'е̄мтыс',\n",
       " '##евна',\n",
       " 'пусмал',\n",
       " '##ович',\n",
       " 'ще̄мья',\n",
       " 'нэ̄пак',\n",
       " 'Наталь',\n",
       " '##евич',\n",
       " 'ӯщлах',\n",
       " 'та̄гыл',\n",
       " 'ла̄тыӈ',\n",
       " '##ве̄н',\n",
       " '##щакв',\n",
       " 'хӯрум',\n",
       " '##уӈкв',\n",
       " 'со̄тыр',\n",
       " '##таль',\n",
       " 'пӯмыщ',\n",
       " '##анум',\n",
       " 'хансым',\n",
       " 'Ка̄сыӈ',\n",
       " '##ӈкве',\n",
       " 'хо̄тпа',\n",
       " 'пуссын',\n",
       " '##овна',\n",
       " 'ва̄рыг',\n",
       " '##упыл',\n",
       " '##льпи',\n",
       " 'тэ̄нут',\n",
       " 'ёмщакв',\n",
       " 'э̄тпос',\n",
       " '##о̄ви',\n",
       " '##осса',\n",
       " '##аныл',\n",
       " 'щё̄пит',\n",
       " 'ма̄гыс',\n",
       " '##квет',\n",
       " '##анув',\n",
       " 'аквъёт',\n",
       " 'та̄рви',\n",
       " 'тэ̄лат',\n",
       " '##маль',\n",
       " '##щтыр',\n",
       " '##сӯв',\n",
       " '##щлах',\n",
       " '##рищ',\n",
       " '##лат',\n",
       " '##нут',\n",
       " 'акван',\n",
       " 'во̄рт',\n",
       " '##ила',\n",
       " '##лыс',\n",
       " 'лю̄ль',\n",
       " 'потыр',\n",
       " '##хат',\n",
       " '##кем',\n",
       " '##ани',\n",
       " '##янэ',\n",
       " '##э̄т',\n",
       " '##мле',\n",
       " '##лум',\n",
       " 'матыр',\n",
       " 'хумус',\n",
       " '##пан',\n",
       " '##сия',\n",
       " '##пыӈ',\n",
       " '##нув',\n",
       " '##тал',\n",
       " '##мат',\n",
       " '##ина',\n",
       " '##̄кв',\n",
       " 'о̄луп',\n",
       " '##е̄т',\n",
       " '##э̄н',\n",
       " 'ма̄нь',\n",
       " '##тыр',\n",
       " '##хо̄',\n",
       " 'э̄рнэ',\n",
       " 'минас',\n",
       " 'нупыл',\n",
       " '##лыг',\n",
       " '##е̄в',\n",
       " '##ант',\n",
       " 'колыт',\n",
       " '##лам',\n",
       " '##е̄м',\n",
       " 'па̄лт',\n",
       " 'ка̄са',\n",
       " 'йӣкв',\n",
       " '##ныг',\n",
       " '##э̄в',\n",
       " 'тэ̄ла',\n",
       " '##а̄г',\n",
       " 'ла̄тӈ',\n",
       " 'тамле',\n",
       " 'тотнэ',\n",
       " '##рум',\n",
       " 'Ха̄ль',\n",
       " '##ниц',\n",
       " '##сыт',\n",
       " 'такви',\n",
       " '##лан',\n",
       " '##тыг',\n",
       " '##нти',\n",
       " '##щхи',\n",
       " '##ӈкв',\n",
       " '##ент',\n",
       " '##пак',\n",
       " '##атэ',\n",
       " '##тан',\n",
       " '##ван',\n",
       " '##а̄н',\n",
       " 'то̄нт',\n",
       " 'а̄тим',\n",
       " 'патыс',\n",
       " '##а̄л',\n",
       " 'нампа',\n",
       " 'Тувыл',\n",
       " 'хульт',\n",
       " '##ква',\n",
       " 'округ',\n",
       " '##ест',\n",
       " 'а̄гит',\n",
       " 'щё̄пи',\n",
       " 'хо̄нт',\n",
       " '##гор',\n",
       " '##тым',\n",
       " '##луп',\n",
       " 'касыл',\n",
       " '##о̄в',\n",
       " 'самын',\n",
       " '##лыщ',\n",
       " '##а̄в',\n",
       " 'ханищ',\n",
       " '##сыг',\n",
       " '##вес',\n",
       " 'ханты',\n",
       " 'та̄лт',\n",
       " '##о̄п',\n",
       " 'акваг',\n",
       " 'та̄ра',\n",
       " '##тыт',\n",
       " '##ве̄',\n",
       " '##лах',\n",
       " '##лас',\n",
       " 'манос',\n",
       " 'о̄ньщ',\n",
       " '##рия',\n",
       " '##нэ̄',\n",
       " '##анэ',\n",
       " 'ёхтыс',\n",
       " '##ъёт',\n",
       " '##вен',\n",
       " '##сын',\n",
       " 'район',\n",
       " '##кру',\n",
       " '##пал',\n",
       " '##лад',\n",
       " '##льт',\n",
       " '##о̄х',\n",
       " 'а̄нум',\n",
       " '##ция',\n",
       " 'ве̄рм',\n",
       " 'олныл',\n",
       " '##пос',\n",
       " 'тувыл',\n",
       " '##алт',\n",
       " '##тыл',\n",
       " '##арт',\n",
       " 'кӯща',\n",
       " 'о̄лыс',\n",
       " 'ва̄ри',\n",
       " 'во̄ра',\n",
       " '##сум',\n",
       " '##лым',\n",
       " '##оль',\n",
       " '##ы̄г',\n",
       " '##ург',\n",
       " '##тап',\n",
       " 'о̄лнэ',\n",
       " '##стэ',\n",
       " '##ьют',\n",
       " 'янмал',\n",
       " '##лын',\n",
       " '##пыг',\n",
       " '##гыл',\n",
       " '##тын',\n",
       " '##сыӈ',\n",
       " 'То̄нт',\n",
       " '##рам',\n",
       " '##увт',\n",
       " '##кве',\n",
       " '##лыт',\n",
       " '##мил',\n",
       " '##ныл',\n",
       " '##щит',\n",
       " '##тыӈ',\n",
       " '##мам',\n",
       " 'ураль',\n",
       " '##лыӈ',\n",
       " 'мо̄йт',\n",
       " 'сосса',\n",
       " '##там',\n",
       " '##э̄г',\n",
       " '##сий',\n",
       " '##кат',\n",
       " '##лыл',\n",
       " 'маныр',\n",
       " 'ялпыӈ',\n",
       " 'порат',\n",
       " '##аве',\n",
       " '##тум',\n",
       " '##а̄т',\n",
       " 'са̄лы',\n",
       " '##рыг',\n",
       " '##иль',\n",
       " 'мирыт',\n",
       " '##сыр',\n",
       " 'хӯнт',\n",
       " '##нав',\n",
       " 'Алекс',\n",
       " '##о̄р',\n",
       " '##ови',\n",
       " '##пат',\n",
       " 'рӯпа',\n",
       " '##ева',\n",
       " '##е̄н',\n",
       " '##вет',\n",
       " '##пас',\n",
       " '##хал',\n",
       " '##тыс',\n",
       " '##кол',\n",
       " 'ща̄нь',\n",
       " 'ма̄хм',\n",
       " '##ков',\n",
       " 'ла̄ви',\n",
       " '##лты',\n",
       " '##вит',\n",
       " 'ро̄ви',\n",
       " '##кам',\n",
       " '##мал',\n",
       " '##онт',\n",
       " '##кан',\n",
       " 'Ма̄нь',\n",
       " '##рис',\n",
       " '##е̄г',\n",
       " '##гум',\n",
       " 'Округ',\n",
       " '##тат',\n",
       " 'хансы',\n",
       " '##итэ',\n",
       " 'Ханты',\n",
       " '##хум',\n",
       " 'о̄йка',\n",
       " 'лё̄ӈх',\n",
       " 'та̄лэ',\n",
       " 'хосыт',\n",
       " '##ман',\n",
       " 'Тамле',\n",
       " '##ова',\n",
       " '##сыл',\n",
       " '##ный',\n",
       " '##гыт',\n",
       " '##ӯв',\n",
       " 'о̄выл',\n",
       " '##тас',\n",
       " '##пыл',\n",
       " 'щирыл',\n",
       " 'Саран',\n",
       " '##пит',\n",
       " '##лов',\n",
       " '##мас',\n",
       " '##тах',\n",
       " 'рӯпи',\n",
       " 'пыгыт',\n",
       " 'Татья',\n",
       " '##ед',\n",
       " '##ия',\n",
       " 'ёхты',\n",
       " 'ма̄х',\n",
       " '##па',\n",
       " '##на',\n",
       " '##сы',\n",
       " '##йт',\n",
       " 'арыг',\n",
       " '##о̄',\n",
       " '##ес',\n",
       " '##ой',\n",
       " '##ыл',\n",
       " '##е̄',\n",
       " '##ым',\n",
       " 'па̄в',\n",
       " 'потр',\n",
       " '##ыщ',\n",
       " '##̄р',\n",
       " '##зе',\n",
       " '##ус',\n",
       " 'хури',\n",
       " 'яныг',\n",
       " 'ӯст',\n",
       " '##ос',\n",
       " '##ун',\n",
       " '##ем',\n",
       " '##ал',\n",
       " '##ья',\n",
       " '##ви',\n",
       " '##̄л',\n",
       " 'Яныг',\n",
       " '##ӈх',\n",
       " '##ом',\n",
       " '##ла',\n",
       " 'таӈх',\n",
       " '##рт',\n",
       " 'наме',\n",
       " 'Са̄в',\n",
       " '##ща',\n",
       " 'ма̄г',\n",
       " '##ск',\n",
       " '##от',\n",
       " '##ъё',\n",
       " '##йп',\n",
       " '##од',\n",
       " '##̄в',\n",
       " 'клас',\n",
       " 'во̄р',\n",
       " '##нь',\n",
       " '##ущ',\n",
       " '##ут',\n",
       " 'са̄в',\n",
       " 'та̄н',\n",
       " '##ту',\n",
       " 'а̄ти',\n",
       " '##де',\n",
       " 'ӯнт',\n",
       " '##ӯ',\n",
       " '##си',\n",
       " '##га',\n",
       " '##са',\n",
       " '##ыӈ',\n",
       " 'нэ̄м',\n",
       " 'солк',\n",
       " 'пуӈк',\n",
       " 'таве',\n",
       " '##те',\n",
       " 'такв',\n",
       " 'халт',\n",
       " 'ма̄н',\n",
       " 'э̄лы',\n",
       " 'ханс',\n",
       " '##тр',\n",
       " 'колн',\n",
       " '##ер',\n",
       " '##нг',\n",
       " '##пи',\n",
       " '##ху',\n",
       " '##ги',\n",
       " 'ла̄х',\n",
       " '##ыг',\n",
       " '##ли',\n",
       " 'рӯп',\n",
       " 'сака',\n",
       " '##ри',\n",
       " '##тэ',\n",
       " '##ыс',\n",
       " '##кв',\n",
       " 'урыл',\n",
       " '##̄м',\n",
       " '##ин',\n",
       " 'музе',\n",
       " 'Влад',\n",
       " 'на̄н',\n",
       " '##та',\n",
       " 'э̄рг',\n",
       " '##лн',\n",
       " 'То̄в',\n",
       " '##ий',\n",
       " 'яныт',\n",
       " '##иг',\n",
       " '##ис',\n",
       " '##кс',\n",
       " '##тӈ',\n",
       " 'ӯрг',\n",
       " 'палт',\n",
       " 'ёхта',\n",
       " '##ат',\n",
       " 'алыщ',\n",
       " 'ялас',\n",
       " '##уп',\n",
       " '##ыр',\n",
       " '##ог',\n",
       " '##ст',\n",
       " '##ир',\n",
       " '##он',\n",
       " 'газе',\n",
       " '##̄г',\n",
       " '##ях',\n",
       " 'пе̄с',\n",
       " '##ах',\n",
       " '##ох',\n",
       " 'пӯм',\n",
       " '##нт',\n",
       " '##оз',\n",
       " '##лу',\n",
       " '##ха',\n",
       " '##ай',\n",
       " '##ей',\n",
       " '##ок',\n",
       " '##ур',\n",
       " '##щи',\n",
       " '##ев',\n",
       " 'ва̄г',\n",
       " 'ще̄м',\n",
       " '##ью',\n",
       " 'хӯл',\n",
       " 'нё̄т',\n",
       " '##ти',\n",
       " '##ет',\n",
       " '##ка',\n",
       " 'колт',\n",
       " 'э̄ри',\n",
       " 'а̄ст',\n",
       " '##ич',\n",
       " 'а̄ги',\n",
       " 'ёмас',\n",
       " '##ве',\n",
       " '##ын',\n",
       " '##ма',\n",
       " '##э̄',\n",
       " 'мо̄т',\n",
       " '##ас',\n",
       " 'та̄л',\n",
       " 'ма̄т',\n",
       " 'по̄с',\n",
       " '##ра',\n",
       " '##ле',\n",
       " '##ты',\n",
       " '##вт',\n",
       " '##̄й',\n",
       " 'во̄в',\n",
       " 'са̄т',\n",
       " '##ки',\n",
       " '##им',\n",
       " '##ам',\n",
       " '##кт',\n",
       " '##ӈк',\n",
       " 'о̄нь',\n",
       " '##ян',\n",
       " '##ль',\n",
       " 'нэ̄г',\n",
       " 'ялан',\n",
       " '##ру',\n",
       " 'ювле',\n",
       " '##̄с',\n",
       " '##уй',\n",
       " 'та̄р',\n",
       " '##ны',\n",
       " '##̄н',\n",
       " 'паты',\n",
       " '##ап',\n",
       " 'ю̄нт',\n",
       " '##̄т',\n",
       " '##ва',\n",
       " '##лт',\n",
       " '##ен',\n",
       " 'хунь',\n",
       " '##ор',\n",
       " '##ик',\n",
       " '##нэ',\n",
       " '##аг',\n",
       " '##кк',\n",
       " '##ил',\n",
       " 'ла̄в',\n",
       " 'ӯсн',\n",
       " '##юм',\n",
       " 'мӯй',\n",
       " '##ол',\n",
       " 'Та̄н',\n",
       " 'ва̄р',\n",
       " '##пр',\n",
       " 'хо̄т',\n",
       " '##ув',\n",
       " '##ум',\n",
       " '##ъя',\n",
       " 'то̄в',\n",
       " '##ов',\n",
       " '##хи',\n",
       " '##ав',\n",
       " 'о̄лы',\n",
       " '##ге',\n",
       " '##ищ',\n",
       " '##рг',\n",
       " 'па̄л',\n",
       " '##ит',\n",
       " '##ар',\n",
       " 'тэ̄н',\n",
       " '##ыт',\n",
       " '##ан',\n",
       " 'а̄гм',\n",
       " 'но̄х',\n",
       " '##рм',\n",
       " 'хо̄н',\n",
       " '##а̄',\n",
       " '##ни',\n",
       " '##ел',\n",
       " '##др',\n",
       " '##ко',\n",
       " 'йӣв',\n",
       " '##ё̄',\n",
       " '##лы',\n",
       " 'Ма̄н',\n",
       " '##ям',\n",
       " '##А',\n",
       " '##ъ',\n",
       " '##ь',\n",
       " 'Рос',\n",
       " 'Тув',\n",
       " 'со̄',\n",
       " 'тов',\n",
       " 'пил',\n",
       " '##Р',\n",
       " '##C',\n",
       " 'о̄й',\n",
       " 'щё̄',\n",
       " 'Са̄',\n",
       " 'акв',\n",
       " '##A',\n",
       " 'а̄т',\n",
       " '##x',\n",
       " 'рай',\n",
       " '##V',\n",
       " 'Хо̄',\n",
       " '##ц',\n",
       " '##S',\n",
       " 'о̄л',\n",
       " 'Ка̄',\n",
       " 'та̄',\n",
       " 'Хан',\n",
       " '##ў',\n",
       " '##С',\n",
       " '##О',\n",
       " 'хар',\n",
       " 'ӯй',\n",
       " '##5',\n",
       " '##1',\n",
       " '##ч',\n",
       " 'пор',\n",
       " 'тах',\n",
       " 'олн',\n",
       " 'рущ',\n",
       " 'хӯ',\n",
       " 'кон',\n",
       " '##б',\n",
       " 'сым',\n",
       " 'тор',\n",
       " 'во̄',\n",
       " 'ла̄',\n",
       " 'кос',\n",
       " '##Д',\n",
       " '##L',\n",
       " '##X',\n",
       " '##л',\n",
       " '##R',\n",
       " '##щ',\n",
       " 'хум',\n",
       " 'мус',\n",
       " '##с',\n",
       " '##o',\n",
       " '##Я',\n",
       " 'сӯ',\n",
       " 'хал',\n",
       " 'Ань',\n",
       " '##¸',\n",
       " '##и',\n",
       " '##I',\n",
       " '##Ӈ',\n",
       " '##f',\n",
       " '##ö',\n",
       " '##Ж',\n",
       " '##м',\n",
       " '##Б',\n",
       " '##ф',\n",
       " 'Там',\n",
       " 'е̄м',\n",
       " '##Ё',\n",
       " '##8',\n",
       " 'Наӈ',\n",
       " '##T',\n",
       " 'Тот',\n",
       " 'вос',\n",
       " '##а',\n",
       " 'лю̄',\n",
       " '##v',\n",
       " 'пат',\n",
       " '##ӈ',\n",
       " '##ӑ',\n",
       " '##Х',\n",
       " 'Ман',\n",
       " 'лӯ',\n",
       " '##М',\n",
       " '##l',\n",
       " '##У',\n",
       " 'мо̄',\n",
       " 'ка̄',\n",
       " 'На̄',\n",
       " '##у',\n",
       " '##К',\n",
       " '##g',\n",
       " 'пӯ',\n",
       " 'ма̄',\n",
       " '##e',\n",
       " 'хот',\n",
       " '##r',\n",
       " 'тох',\n",
       " 'нам',\n",
       " 'ща̄',\n",
       " '##Н',\n",
       " 'о̄в',\n",
       " '##о',\n",
       " '##B',\n",
       " 'кӯ',\n",
       " 'сол',\n",
       " '##г',\n",
       " 'по̄',\n",
       " 'пус',\n",
       " 'А̄с',\n",
       " '##к',\n",
       " 'кас',\n",
       " 'а̄г',\n",
       " 'ань',\n",
       " 'Акв',\n",
       " 'сун',\n",
       " 'ӯр',\n",
       " 'на̄',\n",
       " '##Ч',\n",
       " '##Ш',\n",
       " 'сыс',\n",
       " 'о̄с',\n",
       " '##U',\n",
       " '##П',\n",
       " '##Щ',\n",
       " 'Мо̄',\n",
       " 'ё̄р',\n",
       " '##Ь',\n",
       " '##Ц',\n",
       " 'хур',\n",
       " 'нак',\n",
       " '##M',\n",
       " 'нё̄',\n",
       " '##y',\n",
       " '##w',\n",
       " '##W',\n",
       " '##E',\n",
       " '##O',\n",
       " 'хос',\n",
       " '##4',\n",
       " '##ю',\n",
       " '##P',\n",
       " 'Нэ̄',\n",
       " 'Тат',\n",
       " 'вит',\n",
       " '##h',\n",
       " '##p',\n",
       " '##s',\n",
       " '##D',\n",
       " 'туп',\n",
       " '##є',\n",
       " 'мӯ',\n",
       " '##ә',\n",
       " 'про',\n",
       " '##э',\n",
       " '##Э',\n",
       " 'мощ',\n",
       " '##c',\n",
       " '##п',\n",
       " '##Т',\n",
       " '##з',\n",
       " 'Кон',\n",
       " '##F',\n",
       " '##т',\n",
       " '##в',\n",
       " 'сус',\n",
       " 'хан',\n",
       " 'ва̄',\n",
       " '##2',\n",
       " '##В',\n",
       " '##m',\n",
       " 'пув',\n",
       " '##е',\n",
       " 'сам',\n",
       " 'сэ̄',\n",
       " '##З',\n",
       " '##d',\n",
       " '##™',\n",
       " 'Тав',\n",
       " '##ӆ',\n",
       " '##Й',\n",
       " '##й',\n",
       " '##Е',\n",
       " 'пал',\n",
       " 'а̄н',\n",
       " 'щёс',\n",
       " 'йис',\n",
       " 'о̄щ',\n",
       " 'нас',\n",
       " 'э̄т',\n",
       " 'Ма̄',\n",
       " 'хо̄',\n",
       " 'мин',\n",
       " '##ӊ',\n",
       " '##ä',\n",
       " 'па̄',\n",
       " 'лё̄',\n",
       " '201',\n",
       " '##G',\n",
       " 'нэ̄',\n",
       " 'Ха̄',\n",
       " 'А̄г',\n",
       " '##д',\n",
       " '##Г',\n",
       " 'кол',\n",
       " 'ном',\n",
       " '##K',\n",
       " '##q',\n",
       " '##ó',\n",
       " '##¬',\n",
       " 'кит',\n",
       " 'э̄р',\n",
       " '##a',\n",
       " '##7',\n",
       " 'са̄',\n",
       " 'ӯн',\n",
       " 'мир',\n",
       " 'ё̄м',\n",
       " 'Вас',\n",
       " '##р',\n",
       " '##Y',\n",
       " '##х',\n",
       " 'тав',\n",
       " 'ве̄',\n",
       " 'тыг',\n",
       " '##i',\n",
       " 'ха̄',\n",
       " 'май',\n",
       " '##H',\n",
       " 'тай',\n",
       " '##u',\n",
       " 'тот',\n",
       " '##ш',\n",
       " '##z',\n",
       " 'Хум',\n",
       " 'тэ̄',\n",
       " 'рӯ',\n",
       " 'мас',\n",
       " '##N',\n",
       " '##ё',\n",
       " 'хас',\n",
       " '##3',\n",
       " '##н',\n",
       " 'щар',\n",
       " '##6',\n",
       " '##Ю',\n",
       " 'ойт',\n",
       " '##ü',\n",
       " 'ю̄н',\n",
       " '##Л',\n",
       " '##j',\n",
       " 'пе̄',\n",
       " 'ке̄',\n",
       " '##ы',\n",
       " '##И',\n",
       " '##ѐ',\n",
       " 'Але',\n",
       " 'мат',\n",
       " 'тыт',\n",
       " '##n',\n",
       " '##×',\n",
       " 'ӯс',\n",
       " 'пин',\n",
       " '##t',\n",
       " '##Ы',\n",
       " '##9',\n",
       " '##0',\n",
       " 'пыг',\n",
       " '##Z',\n",
       " '##̄',\n",
       " '##ж',\n",
       " 'тӯ',\n",
       " '##k',\n",
       " '##Ф',\n",
       " 'ёнг',\n",
       " 'наӈ',\n",
       " 'тув',\n",
       " 'ман',\n",
       " 'ёма',\n",
       " '##J',\n",
       " '##я',\n",
       " '##b',\n",
       " '##é',\n",
       " 'Та',\n",
       " 'ми',\n",
       " 'ёт',\n",
       " 'му',\n",
       " 'нё',\n",
       " 'Ня',\n",
       " 'ви',\n",
       " 'Э̄',\n",
       " 'а̄',\n",
       " 'ём',\n",
       " '20',\n",
       " 'юи',\n",
       " 'Лю',\n",
       " 'ур',\n",
       " 'га',\n",
       " 'ту',\n",
       " 'кв',\n",
       " 'я̄',\n",
       " 'ня',\n",
       " 'об',\n",
       " 'хо',\n",
       " 'юв',\n",
       " 'те',\n",
       " 'на',\n",
       " 'ёх',\n",
       " 'ӯ',\n",
       " 'Ку',\n",
       " 'Ни',\n",
       " 'лю',\n",
       " 'пу',\n",
       " 'ос',\n",
       " 'о̄',\n",
       " 'ся',\n",
       " 'щё',\n",
       " 'э̄',\n",
       " 'уч',\n",
       " 'Ма',\n",
       " 'Ху',\n",
       " 'де',\n",
       " 'ю̄',\n",
       " 'ты',\n",
       " 'Юг',\n",
       " 'ян',\n",
       " 'Ты',\n",
       " 'са',\n",
       " 'пр',\n",
       " 'ки',\n",
       " 'мо',\n",
       " 'ёл',\n",
       " 'Ам',\n",
       " 'иӈ',\n",
       " 'Ос',\n",
       " 'ху',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(new_wordpiece.get_vocab(), key=lambda x: len(x), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b874fa0-548f-4cca-9feb-ed45888ef3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TKN_DIR_OLD = '../data/tokenizers/labse_tokenizer'\n",
    "TKN_DIR_NEW = '../data/tokenizers/tokenizer_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca1103e5-2c8d-462c-827b-a689fe0fc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tokens_resave(vocab):\n",
    "    new_tokenizer = deepcopy(labse.tokenizer)\n",
    "    new_tokens = list(set(vocab) - set(new_tokenizer.vocab.keys()))\n",
    "    num_tokens_added = new_tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "    with open(f\"{TKN_DIR_OLD}/tokenizer.json\", \"r\", encoding='utf-8') as f:\n",
    "        cfg = json.load(f)\n",
    "    \n",
    "    cfg['model']['vocab'] = new_tokenizer.vocab\n",
    "    \n",
    "    with open(f\"{TKN_DIR_NEW}/tokenizer.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(cfg, f, indent=False)\n",
    "\n",
    "    with open(f\"{TKN_DIR_NEW}/vocab.txt\", \"w\", encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(list(cfg['model']['vocab'].keys())))\n",
    "    new_tokenizer = AutoTokenizer.from_pretrained(TKN_DIR_NEW, use_fast=False)\n",
    "    print(f'Added {num_tokens_added} new tokens to tokenizer')\n",
    "    #os.remove(TKN_DIR_NEW)\n",
    "    return new_tokenizer, new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "482684d0-4937-42d6-b5a7-ea7cf528d929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 557 new tokens to tokenizer\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer, new_tokens = add_tokens_resave(list(new_wordpiece.get_vocab().keys())) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab2142c5-2b3b-4622-89d3-dc4d0d710d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = AutoTokenizer.from_pretrained('../data/tokenizers/tokenizer_v1', use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df1f021a-7838-4e05-a28d-41c2ab932930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenizers(idx, old_tokenizer=labse.tokenizer, new_tokenizer=new_tokenizer):\n",
    "    source = data.source.loc[idx]\n",
    "    target = data.target.loc[idx]\n",
    "    print('Orig mansi:')\n",
    "    print(target)\n",
    "    #print()\n",
    "    print('Old tokenizer on mansi:')\n",
    "    l1 = old_tokenizer.convert_ids_to_tokens(old_tokenizer.encode(target))\n",
    "    print(l1)\n",
    "    #print()\n",
    "    l11 = old_tokenizer.decode(old_tokenizer.encode(target)[1:-1])\n",
    "    print('New tokenizer on mansi:')\n",
    "    l2 = new_tokenizer.convert_ids_to_tokens(new_tokenizer.encode(target))\n",
    "    print(l2)\n",
    "    #print()\n",
    "    l22 = new_tokenizer.decode(new_tokenizer.encode(target)[1:-1])\n",
    "    print(\"Tokenized equally:\", l1 == l2)\n",
    "    is_equal_decode = l11 == l22 == target\n",
    "    if not is_equal_decode:\n",
    "        print('Decoded not equally')\n",
    "        print('Decoded on old:', l11)\n",
    "        print('Decoded on new:', l22)\n",
    "    else:\n",
    "        print('Decoded equally')\n",
    "    print()\n",
    "    print('Orig rus:')\n",
    "    print(source)\n",
    "    print('Old tokenizer on rus:')\n",
    "    l1 = old_tokenizer.convert_ids_to_tokens(old_tokenizer.encode(source))\n",
    "    print(l1)\n",
    "    l11 = old_tokenizer.decode(old_tokenizer.encode(source)[1:-1])\n",
    "    #print()\n",
    "    print('New tokenizer on rus:')\n",
    "    l2 = new_tokenizer.convert_ids_to_tokens(new_tokenizer.encode(source))\n",
    "    print(l2)\n",
    "    l22 = new_tokenizer.decode(new_tokenizer.encode(source)[1:-1])\n",
    "    #print()\n",
    "    print(\"Tokenized equally:\", l1 == l2)\n",
    "    is_equal_decode = l11 == l22 == source\n",
    "    if not is_equal_decode:\n",
    "        print('Decoded not equally')\n",
    "        print('Decoded on old:', l11)\n",
    "        print('Decoded on new:', l22)\n",
    "    else:\n",
    "        print('Decoded equally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8a4c67a-3ec1-4d70-9649-57e582167b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig mansi:\n",
      "О̄йка сӯнсы воссыг нэ̄матыр ат хультыс, Тыстылэ нупыл ла̄ви: «Ха-та, Тыстыл, ёмщакв уральта̄хтэн, э̄рыӈ тот иӈ олн хультыс?\n",
      "Old tokenizer on mansi:\n",
      "['[CLS]', 'О', '##̄', '##йка', 'су', '##̄', '##нсы', 'во', '##сс', '##ыг', 'н', '##э', '##̄', '##мат', '##ыр', 'ат', 'ху', '##льт', '##ыс', ',', 'Ты', '##сты', '##лэ', 'ну', '##пы', '##л', 'ла', '##̄', '##ви', ':', '«', 'Ха', '-', 'та', ',', 'Ты', '##сты', '##л', ',', 'ём', '##ща', '##к', '##в', 'ура', '##льт', '##а', '##̄', '##хт', '##эн', ',', '[UNK]', 'тот', '[UNK]', 'ол', '##н', 'ху', '##льт', '##ыс', '?', '[SEP]']\n",
      "New tokenizer on mansi:\n",
      "['[CLS]', 'О', '##̄й', '##ка', 'с', '##ӯ', '##нсы', 'вос', '##сыг', 'нэ̄м', '##аты', '##р', 'ат', 'хульт', '##ыс', ',', 'Ты', '##сты', '##лэ', 'нупыл', 'ла̄ви', ':', '«', 'Ха', '-', 'та', ',', 'Ты', '##сты', '##л', ',', 'ёмщакв', 'ураль', '##та', '##̄', '##хт', '##эн', ',', 'э̄р', '##ыӈ', 'тот', 'иӈ', 'олн', 'хульт', '##ыс', '?', '[SEP]']\n",
      "Tokenized equally: False\n",
      "Decoded not equally\n",
      "Decoded on old: О̄йка сӯнсы воссыг нэ̄матыр ат хультыс, Тыстылэ нупыл ла̄ви : « Ха - та, Тыстыл, ёмщакв уральта̄хтэн, [UNK] тот [UNK] олн хультыс?\n",
      "Decoded on new: О̄йка сӯнсы воссыг нэ̄матыр ат хультыс, Тыстылэ нупыл ла̄ви : « Ха - та, Тыстыл, ёмщакв уральта̄хтэн, э̄рыӈ тот иӈ олн хультыс?\n",
      "\n",
      "Orig rus:\n",
      "Видит мужик, что уж больше ничего не осталось, и говорит: « Посмотри-ка, Горе, никак там ещё деньги остались?»\n",
      "Old tokenizer on rus:\n",
      "['[CLS]', 'Види', '##т', 'мужик', ',', 'что', 'уж', 'больше', 'ничего', 'не', 'осталось', ',', 'и', 'говорит', ':', '«', 'По', '##смотр', '##и', '-', 'ка', ',', 'Горе', ',', 'никак', 'там', 'ещё', 'деньги', 'остались', '?', '»', '[SEP]']\n",
      "New tokenizer on rus:\n",
      "['[CLS]', 'Види', '##т', 'мужик', ',', 'что', 'уж', 'больше', 'ничего', 'не', 'осталось', ',', 'и', 'говорит', ':', '«', 'По', '##смотр', '##и', '-', 'ка', ',', 'Горе', ',', 'никак', 'там', 'ещё', 'деньги', 'остались', '?', '»', '[SEP]']\n",
      "Tokenized equally: True\n",
      "Decoded not equally\n",
      "Decoded on old: Видит мужик, что уж больше ничего не осталось, и говорит : « Посмотри - ка, Горе, никак там ещё деньги остались? »\n",
      "Decoded on new: Видит мужик, что уж больше ничего не осталось, и говорит : « Посмотри - ка, Горе, никак там ещё деньги остались? »\n"
     ]
    }
   ],
   "source": [
    "test_tokenizers(6666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec3954-aad8-4ab3-8926-70bf915b50ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35227902-1ef1-406b-ab5c-e6785b06b8c1",
   "metadata": {},
   "source": [
    "# negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b055e7c1-b35f-4598-ba16-fa7e6ce1528a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bde69911c54248a90b1d421e23f989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "negative_rus = []\n",
    "negative_mans = []\n",
    "\n",
    "\n",
    "for dict_scores in tqdm(data['similarities']):\n",
    "    \n",
    "    filtered_items = {k: v for k, v in dict_scores.items() if 0.89 <= v < 0.99}\n",
    "    \n",
    "    if len(filtered_items) > 0:\n",
    "        max_id = max(filtered_items, key=filtered_items.get)\n",
    "        negative_example_rus = data['source'].iloc[max_id]\n",
    "        negative_example_mans = data['target'].iloc[max_id]\n",
    "    else:\n",
    "        negative_example_rus = None\n",
    "        negative_example_mans = None\n",
    "    \n",
    "    negative_rus.append(negative_example_rus)\n",
    "    negative_mans.append(negative_example_mans)\n",
    "    \n",
    "data['negative_examples_mans'] = negative_mans\n",
    "data['negative_examples_rus'] = negative_rus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06bd0bc0-2ef8-410c-90d5-322c3008adbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative_examples_mans\n",
       "None                                                                                                            6951\n",
       "Та̄наныл янытлаӈкв округ кӯщай нэ̄ Наталья Комарова тув ёхталас.                                                 14\n",
       "Тав колхозт, тувыл совхозт рӯпитас.                                                                              13\n",
       "Хоса минас, ва̄ти минас, колэ̄н ёхтыс.                                                                            12\n",
       "Хӯнь во̄ртолнут ӯй хӯл ка̄т ла̄ква - во̄ськасам та̄гмахтум.                                                    12\n",
       "                                                                                                                ... \n",
       "Тав тамле потыртахтын ва̄рмаль мир ёт ос ӯйхулт ёт ва̄рыс.                                                        1\n",
       "Ной пы̄ганэ са̄в ня̄врам о̄сьсыт.                                                                                  1\n",
       "Таима̄гыс То̄рум А̄син та̄н са̄литаве̄сыт, ӯс ат сакватавес.                                                      1\n",
       "Таима̄гыс таве э̄рыс хотум ва̄руӈкве: таве̄н э̄рыс рабыг Египетт хультуӈкве.                                       1\n",
       "Са̄всёс ты ва̄рмаль та̄ра-паттын урыл потырталвес, туп тыит халт нэ̄мхотьют мак э̄рнэ сирыл тохи ат ла̄всас.       1\n",
       "Name: count, Length: 40951, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.negative_examples_mans.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3174ec8-5272-4ca5-91f8-8796261cf51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd483f50-eb77-4b41-afce-a23d085e0e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42abf42c-f6ac-4e1e-bb0c-d12ca3641058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8786e-0e19-440b-ab95-8af2b522f9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv)",
   "language": "python",
   "name": "venv_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
